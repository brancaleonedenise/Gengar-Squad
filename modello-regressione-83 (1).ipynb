{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":107555,"databundleVersionId":13033998,"sourceType":"competition"}],"dockerImageVersionId":31153,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"****Caricamento test.jsonl e train.jsonl****","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport json\nimport warnings\nfrom tqdm.auto import tqdm\n\n# --- 1. Import di Scikit-learn ---\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression   # <-- Il tuo modello\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, RobustScaler # <-- StandardScaler è incluso\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.exceptions import ConvergenceWarning","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T11:45:11.608643Z","iopub.execute_input":"2025-11-13T11:45:11.609081Z","iopub.status.idle":"2025-11-13T11:45:11.623562Z","shell.execute_reply.started":"2025-11-13T11:45:11.609053Z","shell.execute_reply":"2025-11-13T11:45:11.622280Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"try:\n    train_df = pd.read_json('/kaggle/input/fds-pokemon-battles-prediction-2025/train.jsonl', lines=True)\n    test_df = pd.read_json('/kaggle/input/fds-pokemon-battles-prediction-2025/test.jsonl', lines=True)\n    \n    # Convertiamo subito il target in 1 (Vittoria) e 0 (Sconfitta)\n    if 'player_won' in train_df.columns:\n        train_df['player_won'] = train_df['player_won'].astype(int)\n    print(\"Caricamento riuscito!\")\nexcept Exception as e:\n    print(f\"Errore nel caricamento dati: {e}\")\n\n#####\nriga_da_rimuovere = 4877\n\n# Usiamo un controllo per sicurezza, nel caso la riga non esista\nif riga_da_rimuovere in train_df.index:\n    train_df = train_df.drop(riga_da_rimuovere)\n    print(f\"Riga {riga_da_rimuovere} rimossa con successo.\")\nelse:\n    print(f\"Riga {riga_da_rimuovere} non trovata (forse già rimossa o non presente).\")\n\nfiltro_livello_100 = train_df['p1_team_details'].apply(\n    lambda team_list: all(pokemon.get('level') == 100 for pokemon in team_list)\n)\n\ntrain_df = train_df[filtro_livello_100]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T11:45:11.625625Z","iopub.execute_input":"2025-11-13T11:45:11.626064Z","iopub.status.idle":"2025-11-13T11:45:29.514876Z","shell.execute_reply.started":"2025-11-13T11:45:11.626036Z","shell.execute_reply":"2025-11-13T11:45:29.513501Z"}},"outputs":[{"name":"stdout","text":"Caricamento riuscito!\nRiga 4877 rimossa con successo.\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"# Mappa di efficacia dei tipi per la Generazione 1\n# Nota: 'Special' in Gen 1 copre sia Atk Sp. che Def Sp.\n# Non ci sono tipi Dark, Steel, o Fairy.\nTYPE_CHART_GEN1 = {\n    'NORMAL': {'ROCK': 0.5, 'GHOST': 0.0},\n    'FIRE': {'FIRE': 0.5, 'WATER': 0.5, 'GRASS': 2.0, 'ICE': 2.0, 'BUG': 2.0, 'ROCK': 0.5},\n    'WATER': {'FIRE': 2.0, 'WATER': 0.5, 'GRASS': 0.5, 'GROUND': 2.0, 'ROCK': 2.0, 'DRAGON': 0.5},\n    'ELECTRIC': {'WATER': 2.0, 'ELECTRIC': 0.5, 'GRASS': 0.5, 'GROUND': 0.0, 'FLYING': 2.0, 'DRAGON': 0.5},\n    'GRASS': {'FIRE': 0.5, 'WATER': 2.0, 'ELECTRIC': 1.0, 'GRASS': 0.5, 'POISON': 0.5, 'GROUND': 2.0, 'FLYING': 0.5, 'BUG': 0.5, 'ROCK': 2.0, 'DRAGON': 0.5},\n    'ICE': {'WATER': 0.5, 'GRASS': 2.0, 'ICE': 0.5, 'GROUND': 2.0, 'FLYING': 2.0, 'DRAGON': 2.0},\n    'FIGHTING': {'NORMAL': 2.0, 'POISON': 0.5, 'FLYING': 0.5, 'PSYCHIC': 0.5, 'BUG': 0.5, 'ROCK': 2.0, 'GHOST': 0.0},\n    'POISON': {'GRASS': 2.0, 'POISON': 0.5, 'GROUND': 0.5, 'BUG': 2.0, 'ROCK': 0.5, 'GHOST': 0.5},\n    'GROUND': {'FIRE': 2.0, 'ELECTRIC': 2.0, 'GRASS': 0.5, 'POISON': 2.0, 'FLYING': 0.0, 'BUG': 0.5, 'ROCK': 2.0},\n    'FLYING': {'ELECTRIC': 0.5, 'GRASS': 2.0, 'FIGHTING': 2.0, 'BUG': 2.0, 'ROCK': 0.5},\n    'PSYCHIC': {'FIGHTING': 2.0, 'POISON': 2.0, 'PSYCHIC': 0.5, 'GHOST': 1.0}, # In Gen 1, Psychic era immune a Ghost per un bug, ma i dati Showdown potrebbero averlo corretto. Assumiamo 1.0 per sicurezza, o 0.0 se il bug è emulato. Qui usiamo 1.0.\n    'BUG': {'FIRE': 0.5, 'GRASS': 2.0, 'FIGHTING': 0.5, 'POISON': 2.0, 'FLYING': 0.5, 'PSYCHIC': 2.0},\n    'ROCK': {'FIRE': 2.0, 'ICE': 2.0, 'FIGHTING': 0.5, 'GROUND': 0.5, 'FLYING': 2.0, 'BUG': 2.0},\n    'GHOST': {'NORMAL': 0.0, 'PSYCHIC': 0.0, 'GHOST': 2.0}, # Famoso bug: Lick (Ghost) non colpisce Psychic.\n    'DRAGON': {'DRAGON': 2.0},\n}\n\n# Funzione helper per calcolare l'efficacia\ndef get_type_effectiveness(move_type, target_types):\n    if move_type not in TYPE_CHART_GEN1:\n        return 1.0\n    \n    multiplier = 1.0\n    chart_for_move = TYPE_CHART_GEN1[move_type]\n    \n    for target_type in target_types:\n        if target_type in chart_for_move:\n            multiplier *= chart_for_move[target_type]\n            \n    return multiplier\n\n# Pokémon dominanti nel metagame Gen 1 OU (S-Tier e A-Tier)\n# La loro presenza è un segnale fortissimo.\nMETA_THREATS_GEN1 = {\n    'Snorlax', 'Tauros', 'Chansey', 'Alakazam', 'Starmie', 'Exeggutor', \n    'Zapdos', 'Jolteon', 'Rhydon', 'Golem', 'Lapras'\n}\n\n# Mosse di setup o status chiave\nSTATUS_MOVES = {'Thunder Wave', 'Sleep Powder', 'Sing', 'Toxic', 'Lovely Kiss', 'Spore', 'Stun Spore', 'Glare'}\nSETUP_MOVES = {'Amnesia', 'Swords Dance', 'Agility', 'Growth'}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T11:45:29.516594Z","iopub.execute_input":"2025-11-13T11:45:29.516936Z","iopub.status.idle":"2025-11-13T11:45:29.529587Z","shell.execute_reply.started":"2025-11-13T11:45:29.516908Z","shell.execute_reply":"2025-11-13T11:45:29.528208Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"def create_advanced_features(df):\n    processed_data = []\n    for _, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Creazione features\"):\n        p1_team = row['p1_team_details']\n        p2_lead = row['p2_lead_details']\n        timeline = row['battle_timeline']\n        p1_lead = p1_team[0]\n        \n        feat_lead_speed_diff = p1_lead['base_spe'] - p2_lead['base_spe']\n        \n        p1_seen_status = {p['name']: {'hp_pct': 100, 'status': None} for p in p1_team}\n        p2_seen_status = {p2_lead['name']: {'hp_pct': 100, 'status': None}}\n        \n        feat_end_boost_diff = 0\n        feat_num_turns = 0\n        \n        if timeline:\n            feat_num_turns = timeline[-1].get('turn', 0)\n            for turn in timeline:\n                p1_state = turn.get('p1_pokemon_state')\n                if p1_state and p1_state.get('name'):\n                    p1_name = p1_state['name']\n                    p1_seen_status.setdefault(p1_name, {'hp_pct': 100, 'status': None})\n                    p1_seen_status[p1_name]['hp_pct'] = p1_state.get('hp_pct', p1_seen_status[p1_name]['hp_pct'])\n                    p1_seen_status[p1_name]['status'] = p1_state.get('status', p1_seen_status[p1_name]['status'])\n                    \n                p2_state = turn.get('p2_pokemon_state')\n                if p2_state and p2_state.get('name'):\n                    p2_name = p2_state['name']\n                    p2_seen_status.setdefault(p2_name, {'hp_pct': 100, 'status': None})\n                    p2_seen_status[p2_name]['hp_pct'] = p2_state.get('hp_pct', p2_seen_status[p2_name]['hp_pct'])\n                    p2_seen_status[p2_name]['status'] = p2_state.get('status', p2_seen_status[p2_name]['status'])\n\n                if turn.get('turn') == feat_num_turns:\n                    p1_boosts = sum(p1_state.get('boosts', {}).values()) if p1_state else 0\n                    p2_boosts = sum(p2_state.get('boosts', {}).values()) if p2_state else 0\n                    feat_end_boost_diff = p1_boosts - p2_boosts\n\n        p1_total_hp_seen = sum(p['hp_pct'] for p in p1_seen_status.values())\n        p2_total_hp_seen = sum(p['hp_pct'] for p in p2_seen_status.values())\n        feat_hp_advantage_seen = p1_total_hp_seen - p2_total_hp_seen\n        \n        feat_mons_revealed_diff = len(p2_seen_status) - len(p1_seen_status)\n        \n        p1_team_status_count = sum(1 for p in p1_seen_status.values() if p['status'] is not None)\n        p2_team_status_count = sum(1 for p in p2_seen_status.values() if p['status'] is not None)\n        feat_team_status_diff = p1_team_status_count - p2_team_status_count # (P1 status) - (P2 status)\n\n        processed_data.append({\n            'battle_id': row['battle_id'],\n            'p1_lead_name': p1_lead['name'], 'p2_lead_name': p2_lead['name'],\n            'lead_speed_diff': feat_lead_speed_diff,\n            'hp_advantage_seen': feat_hp_advantage_seen,\n            'mons_revealed_diff': feat_mons_revealed_diff,\n            'team_status_diff': feat_team_status_diff,\n            'end_boost_diff': feat_end_boost_diff,\n            'num_turns': feat_num_turns\n        })\n    return pd.DataFrame(processed_data).set_index('battle_id')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T11:45:29.531279Z","iopub.execute_input":"2025-11-13T11:45:29.531806Z","iopub.status.idle":"2025-11-13T11:45:29.566799Z","shell.execute_reply.started":"2025-11-13T11:45:29.531771Z","shell.execute_reply":"2025-11-13T11:45:29.565689Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"print(\"Inizio feature engineering avanzata sul set di training...\")\nX_train_features = create_advanced_features(train_df)\n\nprint(\"\\nInizio feature engineering avanzata sul set di test...\")\nX_test_features = create_advanced_features(test_df)\n\n# Definiamo la nostra variabile target 'y'\ny_train = train_df.set_index('battle_id')['player_won']\n\n# Allineiamo X e y\ny_train = y_train.loc[X_train_features.index]\n\nprint(\"\\nFeature engineering completato. Esempio di dati trasformati:\")\nprint(X_train_features.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T11:45:29.569522Z","iopub.execute_input":"2025-11-13T11:45:29.570069Z","iopub.status.idle":"2025-11-13T11:45:31.889948Z","shell.execute_reply.started":"2025-11-13T11:45:29.570043Z","shell.execute_reply":"2025-11-13T11:45:31.888771Z"}},"outputs":[{"name":"stdout","text":"Inizio feature engineering avanzata sul set di training...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Creazione features:   0%|          | 0/9996 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e8981e88f684e76b87231306b71c175"}},"metadata":{}},{"name":"stdout","text":"\nInizio feature engineering avanzata sul set di test...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Creazione features:   0%|          | 0/5000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18e323463c0341c8bf903e314944aa57"}},"metadata":{}},{"name":"stdout","text":"\nFeature engineering completato. Esempio di dati trasformati:\n          p1_lead_name p2_lead_name  lead_speed_diff  hp_advantage_seen  \\\nbattle_id                                                                 \n0              starmie      starmie                0         201.225312   \n1                 jynx     alakazam              -25          -0.990000   \n2            exeggutor      chansey                5         299.020000   \n3               gengar       tauros                0         100.180000   \n4             alakazam      starmie                5         100.610000   \n\n           mons_revealed_diff  team_status_diff  end_boost_diff  num_turns  \nbattle_id                                                                   \n0                          -2                 0               0         30  \n1                           0                 0               2         30  \n2                          -2                -1               0         30  \n3                          -2                 1               0         30  \n4                          -1                 0               0         30  \n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"numeric_features = [\n    'lead_speed_diff',\n    'hp_advantage_seen',\n    'mons_revealed_diff',\n    'team_status_diff',\n    'end_boost_diff',\n    'num_turns'\n]\ncategorical_features = ['p1_lead_name', 'p2_lead_name']\n\n# Creiamo i trasformatori (StandardScaler e OneHotEncoder)\nnumeric_transformer = Pipeline(steps=[('scaler', RobustScaler())])\ncategorical_transformer = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features),\n        ('cat', categorical_transformer, categorical_features)\n    ],\n    remainder='passthrough'\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T11:46:39.651426Z","iopub.execute_input":"2025-11-13T11:46:39.652051Z","iopub.status.idle":"2025-11-13T11:46:39.658577Z","shell.execute_reply.started":"2025-11-13T11:46:39.652021Z","shell.execute_reply":"2025-11-13T11:46:39.657164Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"\n# Dividiamo i dati di training per una validazione locale\nX_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n    X_train_features, \n    y_train, \n    test_size=0.25, # 20% per la validazione\n    random_state=42,\n    stratify=y_train # Mantiene l'equilibrio delle classi\n)\n\nprint(f\"Dimensione Training Split: {X_train_split.shape}\")\nprint(f\"Dimensione Validation Split: {X_val_split.shape}\")\n\n# 1. Creiamo la pipeline con un modello \"di default\"\n# Usiamo C=1.0 come valore predefinito\nbaseline_pipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('classifier', LogisticRegression(random_state=42, max_iter=1000, solver='liblinear', C=1.0))\n])\n\n# 2. Alleniamo il modello base SUL SOLO SET DI TRAINING SPLIT\nprint(\"\\nAllenamento del modello baseline...\")\nbaseline_pipeline.fit(X_train_split, y_train_split)\n\n# 3. Valutiamo il modello base SUL SET DI VALIDAZIONE\ny_val_pred = baseline_pipeline.predict(X_val_split)\nval_accuracy = accuracy_score(y_val_split, y_val_pred)\n\nprint(f\"\\n--- Risultati Modello Baseline ---\")\nprint(f\"Accuracy sul Validation Set: {val_accuracy:.4f}\")\nprint(\"---------------------------------\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T11:46:56.693529Z","iopub.execute_input":"2025-11-13T11:46:56.693897Z","iopub.status.idle":"2025-11-13T11:46:56.789667Z","shell.execute_reply.started":"2025-11-13T11:46:56.693870Z","shell.execute_reply":"2025-11-13T11:46:56.788694Z"}},"outputs":[{"name":"stdout","text":"Dimensione Training Split: (7497, 8)\nDimensione Validation Split: (2499, 8)\n\nAllenamento del modello baseline...\n\n--- Risultati Modello Baseline ---\nAccuracy sul Validation Set: 0.7407\n---------------------------------\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nprint(\"\\nAvvio di GridSearchCV per l'ottimizzazione degli iperparametri...\")\n# 1. Creiamo la pipeline (la stessa di prima, ma senza 'C' definito)\n# La pipeline che verrà testata da GridSearchCV\ntuning_pipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('classifier', LogisticRegression(random_state=42, max_iter=1000, solver='liblinear'))\n])\n\n# 2. Definiamo la griglia dei parametri\n# Vogliamo testare diversi valori per 'classifier__C'\nparam_grid = {\n    'classifier__penalty': ['l1','l2'], \n    'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n    'classifier__solver': ['liblinear'] \n}\n\n# 3. Impostiamo GridSearchCV\n# cv=5 significa 5-fold cross-validation\n# scoring='accuracy' è la nostra metrica\n# n_jobs=-1 usa tutti i processori\ngrid_search = GridSearchCV(\n    tuning_pipeline, \n    param_grid, \n    cv=10, \n    scoring='accuracy', \n    n_jobs=-1,\n    verbose=1 # Mostra i log\n)\n\n# 4. Avviamo la ricerca sull'INTERO set di training\n# (GridSearchCV gestirà internamente le divisioni di cross-validation)\ngrid_search.fit(X_train_features, y_train)\n\n# 5. Analizziamo i risultati\nprint(\"\\n--- Risultati GridSearchCV ---\")\nprint(f\"Migliori parametri trovati: {grid_search.best_params_}\")\nprint(f\"Migliore Accuracy (media CV): {grid_search.best_score_:.4f}\")\nprint(\"------------------------------\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T11:51:02.598656Z","iopub.execute_input":"2025-11-13T11:51:02.599775Z","iopub.status.idle":"2025-11-13T11:54:28.961023Z","shell.execute_reply.started":"2025-11-13T11:51:02.599723Z","shell.execute_reply":"2025-11-13T11:54:28.959977Z"}},"outputs":[{"name":"stdout","text":"\nAvvio di GridSearchCV per l'ottimizzazione degli iperparametri...\nFitting 10 folds for each of 14 candidates, totalling 140 fits\n\n--- Risultati GridSearchCV ---\nMigliori parametri trovati: {'classifier__C': 1000, 'classifier__penalty': 'l1', 'classifier__solver': 'liblinear'}\nMigliore Accuracy (media CV): 0.8277\n------------------------------\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"import pandas as pd\n\n# 1. Estrai il modello migliore (la pipeline completa) da GridSearchCV\nfinal_model = grid_search.best_estimator_\n\n# 2. Estrai i nomi delle feature numeriche (li abbiamo già)\n# Assicurati che 'numeric_features' sia la lista aggiornata che hai usato\n# numeric_features = ['lead_speed_diff', 'p1_lead_type_adv', ...]\n\n# 3. Estrai i nomi delle feature categoriche create dal OneHotEncoder\n# Questo è il passaggio chiave\ntry:\n    categorical_names = final_model.named_steps['preprocessor'] \\\n                                     .named_transformers_['cat'] \\\n                                     .named_steps['onehot'] \\\n                                     .get_feature_names_out(categorical_features)\nexcept AttributeError:\n    # Fallback per versioni più vecchie di scikit-learn\n    categorical_names = final_model.named_steps['preprocessor'] \\\n                                     .named_transformers_['cat'] \\\n                                     .named_steps['onehot'] \\\n                                     .get_feature_names_out()\n\n# 4. Combina tutti i nomi delle feature nell'ordine corretto\nall_feature_names = numeric_features + list(categorical_names)\n\n# 5. Estrai i coefficienti (l'importanza) dal modello di regressione logistica\ncoefficients = final_model.named_steps['classifier'].coef_[0]\n\n# 6. Crea un DataFrame per visualizzarli in modo chiaro\nimportance_df = pd.DataFrame({\n    'Feature': all_feature_names,\n    'Coefficient': coefficients\n})\n\n# 7. Aggiungi il 'Coefficiente Assoluto' per ordinare per impatto (sia positivo che negativo)\nimportance_df['Impact'] = importance_df['Coefficient'].abs()\nimportance_df = importance_df.sort_values(by='Impact', ascending=False)\n\n# 8. Stampa i risultati\nprint(\"--- Importanza delle Feature (Coefficienti del Modello) ---\")\nprint(importance_df.to_string()) # .to_string() stampa tutto il DataFrame senza troncamenti","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T11:46:28.741153Z","iopub.status.idle":"2025-11-13T11:46:28.741528Z","shell.execute_reply.started":"2025-11-13T11:46:28.741345Z","shell.execute_reply":"2025-11-13T11:46:28.741363Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1. Estrai il modello finale\nfinal_model = grid_search.best_estimator_\n\n# 2. Genera le predizioni (saranno True/False)\ntest_predictions_bool = final_model.predict(X_test_features)\n\n# 3. --- CORREZIONE 1: Converti True/False in 1/0 ---\n# .astype(int) converte True -> 1 e False -> 0\ntest_predictions_int = test_predictions_bool.astype(int)\n\n# 4. Prendi i battle_id dall'indice\ntest_battle_ids = X_test_features.index\n\n# 5. Crea il DataFrame con le due colonne CORRETTE\nsubmission_df = pd.DataFrame({\n    'battle_id': test_battle_ids,\n    'player_won': test_predictions_int  # Usa la versione 1/0\n})\n\n# 6. --- CORREZIONE 2: Salva SENZA l'indice di pandas ---\n# Aggiungendo 'index=False' si risolve il problema della \"stessa colonna\".\nsubmission_df.to_csv('submission_predictions.csv', index=False)\n\nprint(\"\\n-------------------------------------------------\")\nprint(\"File 'submission_predictions.csv' creato con successo!\")\nprint(\"Ora conterrà 1 e 0, e colonne separate.\")\nprint(\"-------------------------------------------------\")\n\n# Stampa un'anteprima\nprint(submission_df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T11:54:34.692899Z","iopub.execute_input":"2025-11-13T11:54:34.693738Z","iopub.status.idle":"2025-11-13T11:54:34.725152Z","shell.execute_reply.started":"2025-11-13T11:54:34.693701Z","shell.execute_reply":"2025-11-13T11:54:34.723936Z"}},"outputs":[{"name":"stdout","text":"\n-------------------------------------------------\nFile 'submission_predictions.csv' creato con successo!\nOra conterrà 1 e 0, e colonne separate.\n-------------------------------------------------\n   battle_id  player_won\n0          0           0\n1          1           1\n2          2           1\n3          3           1\n4          4           1\n","output_type":"stream"}],"execution_count":40}]}