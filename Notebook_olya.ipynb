{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f96f89ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    'lead_speed_diff',\n",
    "    'hp_advantage_seen','mons_revealed_diff','team_status_diff','end_boost_diff',\n",
    "    'total_damage_dealt','total_healing_done','status_turns',\n",
    "    'first_faint_turn','total_stats_diff','damage_diff_turn10',\n",
    "    'damage_diff_turn20','damage_diff_turn25','damage_diff_turn30',\n",
    "    'hp_trend_diff','feat_switch_diff','feat_aggression_diff','hp_diff_std',\n",
    "    'hp_diff_range','momentum_shift_turn','comeback_score','early_sustain',\n",
    "    'status_balance','boost_volatility','boost_trend','move_power_diff',\n",
    "    'move_diversity_diff','stall_ratio','aggression_index',\n",
    "    'stats_speed_interaction',\n",
    "    'hp_vs_stats_ratio','damage_ratio_turn25_30','damage_ratio_turn20_25',\n",
    "    'damage_ratio_turn10_20','damage_ratio_turn10_30',\n",
    "    'atk_def_ratio_p1','atk_def_ratio_p2','hp_speed_interaction_lead','hp_def_ratio_p1',\n",
    "    'hp_def_ratio_p2','p1_hp_mean','p2_hp_mean','hp_diff_mean','hp_diff_last',\n",
    "    'p1_boost_mean','p2_boost_mean','boost_diff_mean','p1_status_total',\n",
    "    'p2_status_total','momentum_flips','p1_aggression','p2_aggression',\n",
    "    'aggression_diff','feat_team_emb_sim',\n",
    "    'lead_type_adv','meta_diff','feat_status_diff_inflicted','status_setup_diff',\n",
    "    \n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87de3525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Local environment detected. Loading data from: Data\n",
      "Riga 4877 rimossa con successo.\n",
      "✓ train.jsonl loaded successfully. Shape: (9996, 5)\n",
      "✓ test.jsonl loaded successfully. Shape: (5000, 4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a280229d6e4644c3aed33434f5534bb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating advanced features:   0%|          | 0/9996 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3267ebd0eba4c629f72df14a1f74a8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating advanced features:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:\n",
      "(7996, 84) (2000, 84) (7996,) (2000,)\n"
     ]
    }
   ],
   "source": [
    "from main import load_data\n",
    "from Features.features_olya import create_advanced_features_gen2\n",
    "import os\n",
    "import pandas as pd\n",
    "from utils.load_json import load_jsonl\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "test_size=0.2\n",
    "random_state=42\n",
    "train_df, test_df = load_data()\n",
    "\n",
    "# Feature engineering\n",
    "X_train_features = create_advanced_features_gen2(train_df)\n",
    "X_test_features = create_advanced_features_gen2(test_df)\n",
    "\n",
    "# Target\n",
    "y_train = train_df.set_index('battle_id')['player_won']\n",
    "\n",
    "# Train/val split\n",
    "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
    "    X_train_features,\n",
    "    y_train,\n",
    "    test_size=test_size,\n",
    "    random_state=random_state,\n",
    "    stratify=y_train\n",
    ")\n",
    "\n",
    "print(\"Shapes:\")\n",
    "print(X_train_split.shape, X_val_split.shape, y_train_split.shape, y_val_split.shape)\n",
    "\n",
    "\n",
    "# Now you can create pipelines and call optimizers\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4f075b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Models.pipeline import get_pipeline\n",
    "\n",
    "\"\"\"\n",
    "Available models and recommended scaler usage:\n",
    "\n",
    "1. Logistic Regression ('logistic')\n",
    "   - Recommended scaler: RobustScaler (default 'auto')\n",
    "   - Key parameters: C, penalty ('l1', 'l2'), class_weight\n",
    "\n",
    "2. Random Forest ('random_forest')\n",
    "   - Recommended scaler: RobustScaler (default 'auto')\n",
    "   - Key parameters: n_estimators, max_depth, min_samples_split, min_samples_leaf, max_features\n",
    "\n",
    "3. XGBoost ('xgboost')\n",
    "   - Recommended scaler: RobustScaler (default 'auto')\n",
    "   - Key parameters: n_estimators, max_depth, learning_rate, subsample, colsample_bytree, gamma\n",
    "\n",
    "4. LightGBM ('lightgbm')\n",
    "   - Recommended scaler: StandardScaler (default 'auto')\n",
    "   - Key parameters: n_estimators, num_leaves, learning_rate, max_depth, feature_fraction, bagging_fraction, min_child_samples, lambda_l1, lambda_l2\n",
    "\n",
    "5. CatBoost ('catboost')\n",
    "   - Recommended scaler: StandardScaler (default 'auto')\n",
    "   - Key parameters: depth, learning_rate, iterations, l2_leaf_reg, random_seed, task_type\n",
    "\n",
    "6. Gradient Boosting ('gradient_boost')\n",
    "   - Recommended scaler: RobustScaler (default 'auto')\n",
    "   - Key parameters: n_estimators, max_depth, learning_rate, min_samples_split, min_samples_leaf, subsample\n",
    "\"\"\"\n",
    "\n",
    "from Models.pipeline import get_pipeline\n",
    "\n",
    "\"\"\"\n",
    "Pipelines for all supported models.\n",
    "Each pipeline is created with the recommended scaler unless you override it.\n",
    "\"\"\"\n",
    "\n",
    "# 1. Logistic Regression\n",
    "pipeline_logistic = get_pipeline(\n",
    "    model_name='logistic',\n",
    "    numerical_features=features,\n",
    "    scaler='robust'   # recommended\n",
    ")\n",
    "\n",
    "# 2. Random Forest\n",
    "pipeline_random_forest = get_pipeline(\n",
    "    model_name='random_forest',\n",
    "    numerical_features=features,\n",
    "    scaler='robust'   # recommended\n",
    ")\n",
    "\n",
    "# 3. XGBoost\n",
    "pipeline_xgb = get_pipeline(\n",
    "    model_name='xgboost',\n",
    "    numerical_features=features,\n",
    "    scaler='robust'   # recommended\n",
    ")\n",
    "\n",
    "# 4. LightGBM\n",
    "pipeline_lgbm = get_pipeline(\n",
    "    model_name='lightgbm',\n",
    "    numerical_features=features,\n",
    "    scaler='standard'  # recommended\n",
    ")\n",
    "\n",
    "# 5. CatBoost\n",
    "pipeline_catboost = get_pipeline(\n",
    "    model_name='catboost',\n",
    "    numerical_features=features,\n",
    "    scaler='standard'  # recommended\n",
    ")\n",
    "\n",
    "# 6. Gradient Boosting\n",
    "pipeline_gradient_boost = get_pipeline(\n",
    "    model_name='gradient_boost',\n",
    "    numerical_features=features,\n",
    "    scaler='robust'   # recommended\n",
    ")\n",
    "\n",
    "# If you want all of them in one dict:\n",
    "all_pipelines = {\n",
    "    \"logistic\": pipeline_logistic,\n",
    "    \"random_forest\": pipeline_random_forest,\n",
    "    \"xgboost\": pipeline_xgb,\n",
    "    \"lightgbm\": pipeline_lgbm,\n",
    "    \"catboost\": pipeline_catboost,\n",
    "    \"gradient_boost\": pipeline_gradient_boost\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d3a380a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training logistic...\n",
      "logistic validation accuracy: 0.7700\n",
      "\n",
      "Training random_forest...\n",
      "random_forest validation accuracy: 0.8130\n",
      "\n",
      "Training xgboost...\n",
      "xgboost validation accuracy: 0.8080\n",
      "\n",
      "Training lightgbm...\n",
      "[LightGBM] [Info] Number of positive: 3998, number of negative: 3998\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002132 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5808\n",
      "[LightGBM] [Info] Number of data points in the train set: 7996, number of used features: 44\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\baudz\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lightgbm validation accuracy: 0.8065\n",
      "\n",
      "Training catboost...\n",
      "catboost validation accuracy: 0.8175\n",
      "\n",
      "Training gradient_boost...\n",
      "gradient_boost validation accuracy: 0.8120\n",
      "\n",
      "Summary of results:\n",
      "logistic: 0.7700\n",
      "random_forest: 0.8130\n",
      "xgboost: 0.8080\n",
      "lightgbm: 0.8065\n",
      "catboost: 0.8175\n",
      "gradient_boost: 0.8120\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Prepare data (if you need to drop columns, do it here)\n",
    "X_train_clean = X_train_split\n",
    "X_val_clean = X_val_split\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, pipeline in all_pipelines.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "\n",
    "    # Fit\n",
    "    pipeline.fit(X_train_clean, y_train_split)\n",
    "\n",
    "    # Predict\n",
    "    preds = pipeline.predict(X_val_clean)\n",
    "\n",
    "    # Evaluate\n",
    "    acc = accuracy_score(y_val_split, preds)\n",
    "    results[name] = acc\n",
    "\n",
    "    print(f\"{name} validation accuracy: {acc:.4f}\")\n",
    "\n",
    "print(\"\\nSummary of results:\")\n",
    "for name, acc in results.items():\n",
    "    print(f\"{name}: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21fc8c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LOGISTIC ===\n",
      "Validation Accuracy: 0.7700\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.77      0.76      0.77      1000\n",
      "        True       0.77      0.78      0.77      1000\n",
      "\n",
      "    accuracy                           0.77      2000\n",
      "   macro avg       0.77      0.77      0.77      2000\n",
      "weighted avg       0.77      0.77      0.77      2000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[761 239]\n",
      " [221 779]]\n",
      "\n",
      "=== RANDOM_FOREST ===\n",
      "Validation Accuracy: 0.8130\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.80      0.83      0.82      1000\n",
      "        True       0.82      0.80      0.81      1000\n",
      "\n",
      "    accuracy                           0.81      2000\n",
      "   macro avg       0.81      0.81      0.81      2000\n",
      "weighted avg       0.81      0.81      0.81      2000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[829 171]\n",
      " [203 797]]\n",
      "\n",
      "=== XGBOOST ===\n",
      "Validation Accuracy: 0.8080\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.80      0.82      0.81      1000\n",
      "        True       0.81      0.80      0.81      1000\n",
      "\n",
      "    accuracy                           0.81      2000\n",
      "   macro avg       0.81      0.81      0.81      2000\n",
      "weighted avg       0.81      0.81      0.81      2000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[818 182]\n",
      " [202 798]]\n",
      "\n",
      "=== LIGHTGBM ===\n",
      "[LightGBM] [Info] Number of positive: 3998, number of negative: 3998\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001533 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5808\n",
      "[LightGBM] [Info] Number of data points in the train set: 7996, number of used features: 44\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\baudz\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\baudz\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8065\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.80      0.82      0.81      1000\n",
      "        True       0.82      0.79      0.80      1000\n",
      "\n",
      "    accuracy                           0.81      2000\n",
      "   macro avg       0.81      0.81      0.81      2000\n",
      "weighted avg       0.81      0.81      0.81      2000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[822 178]\n",
      " [209 791]]\n",
      "\n",
      "=== CATBOOST ===\n",
      "Validation Accuracy: 0.8175\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.81      0.82      0.82      1000\n",
      "        True       0.82      0.81      0.82      1000\n",
      "\n",
      "    accuracy                           0.82      2000\n",
      "   macro avg       0.82      0.82      0.82      2000\n",
      "weighted avg       0.82      0.82      0.82      2000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[824 176]\n",
      " [189 811]]\n",
      "\n",
      "=== GRADIENT_BOOST ===\n",
      "Validation Accuracy: 0.8120\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.81      0.81      0.81      1000\n",
      "        True       0.81      0.81      0.81      1000\n",
      "\n",
      "    accuracy                           0.81      2000\n",
      "   macro avg       0.81      0.81      0.81      2000\n",
      "weighted avg       0.81      0.81      0.81      2000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[809 191]\n",
      " [185 815]]\n",
      "\n",
      "=== SUMMARY ===\n",
      "logistic: 0.7700\n",
      "random_forest: 0.8130\n",
      "xgboost: 0.8080\n",
      "lightgbm: 0.8065\n",
      "catboost: 0.8175\n",
      "gradient_boost: 0.8120\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "X_train_clean = X_train_split\n",
    "X_val_clean   = X_val_split\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, pipeline in all_pipelines.items():\n",
    "    print(f\"\\n=== {name.upper()} ===\")\n",
    "\n",
    "    # Fit\n",
    "    pipeline.fit(X_train_clean, y_train_split)\n",
    "\n",
    "    # Predict labels\n",
    "    preds = pipeline.predict(X_val_clean)\n",
    "\n",
    "    # Predict probabilities (if available)\n",
    "    try:\n",
    "        probs = pipeline.predict_proba(X_val_clean)[:, 1]\n",
    "    except:\n",
    "        probs = None\n",
    "\n",
    "    # Accuracy\n",
    "    acc = accuracy_score(y_val_split, preds)\n",
    "    results[name] = acc\n",
    "    print(f\"Validation Accuracy: {acc:.4f}\")\n",
    "\n",
    "    # Classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_val_split, preds))\n",
    "\n",
    "    # Confusion matrix\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_val_split, preds))\n",
    "\n",
    "    # Store predictions & probs if you want to use later\n",
    "    results[name] = {\n",
    "        \"accuracy\": acc,\n",
    "        \"preds\": preds,\n",
    "        \"probs\": probs\n",
    "    }\n",
    "\n",
    "print(\"\\n=== SUMMARY ===\")\n",
    "for name, info in results.items():\n",
    "    print(f\"{name}: {info['accuracy']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "032232a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from paramethers.cat_grid import param_grid as catboost_param_grid\n",
    "from paramethers.gb_grid import param_grid as gradientboost_param_grid\n",
    "from paramethers.lgb_grid import param_grid as lightgbm_param_grid\n",
    "from paramethers.log_grid import param_grid as logistic_param_grid\n",
    "from paramethers.rf_grid import param_grid as randomforest_param_grid\n",
    "from paramethers.xgb_grid import param_grid as xgboost_param_grid\n",
    "\n",
    "\n",
    "from optimisers.gridsearch_optimizer import run_grid_search\n",
    "from optimisers.optuna_optimizer import optimize_optuna\n",
    "from optimisers.randomsearch_optimizer import run_random_search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8135246b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier__boosting_type': ['dart'],\n",
       " 'classifier__n_estimators': [400, 500, 600, 700, 800],\n",
       " 'classifier__learning_rate': [0.01, 0.03, 0.05, 0.08, 0.1, 0.15, 0.2],\n",
       " 'classifier__num_leaves': [31, 63, 127, 255],\n",
       " 'classifier__max_depth': [-1, 3, 5, 8, 10, 12],\n",
       " 'classifier__feature_fraction': [0.7, 0.8, 0.9, 1.0],\n",
       " 'classifier__bagging_fraction': [0.7, 0.8, 0.9, 1.0],\n",
       " 'classifier__min_child_samples': [5, 10, 20, 30, 50],\n",
       " 'classifier__lambda_l1': [0.001, 0.01, 0.1, 1.0, 5.0, 10.0],\n",
       " 'classifier__lambda_l2': [0.001, 0.01, 0.1, 1.0, 5.0, 10.0]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgboost_param_grid\n",
    "catboost_param_grid\n",
    "gradientboost_param_grid\n",
    "lightgbm_param_grid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd523cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[00:16:08] INFO: Starting Optuna optimization for 1 trials...\n",
      "[I 2025-11-15 00:16:08,080] A new study created in memory with name: no-name-679fcad9-cf96-4549-9eb0-b3193469ef6e\n",
      "[00:16:09] INFO: Trial 1/1 - Accuracy: 0.8185 - Params: {'classifier__max_depth': 8, 'classifier__learning_rate': 0.01, 'classifier__subsample': 0.8, 'classifier__colsample_bytree': 0.8, 'classifier__min_child_weight': 10, 'classifier__gamma': 1, 'classifier__reg_alpha': 1, 'classifier__reg_lambda': 0.001, 'classifier__n_estimators': 200}\n",
      "[I 2025-11-15 00:16:09,111] Trial 0 finished with value: 0.8185 and parameters: {'classifier__max_depth': 8, 'classifier__learning_rate': 0.01, 'classifier__subsample': 0.8, 'classifier__colsample_bytree': 0.8, 'classifier__min_child_weight': 10, 'classifier__gamma': 1, 'classifier__reg_alpha': 1, 'classifier__reg_lambda': 0.001, 'classifier__n_estimators': 200}. Best is trial 0 with value: 0.8185.\n",
      "[00:16:09] INFO: Optimization finished. Best Accuracy: 0.8185\n",
      "[00:16:09] INFO: Best Parameters: {'classifier__max_depth': 8, 'classifier__learning_rate': 0.01, 'classifier__subsample': 0.8, 'classifier__colsample_bytree': 0.8, 'classifier__min_child_weight': 10, 'classifier__gamma': 1, 'classifier__reg_alpha': 1, 'classifier__reg_lambda': 0.001, 'classifier__n_estimators': 200}\n",
      "[00:16:09] INFO: Fitting the best pipeline on the provided training data...\n",
      "[00:16:09] INFO: Starting Optuna optimization for 1 trials...\n",
      "[I 2025-11-15 00:16:09,894] A new study created in memory with name: no-name-37aa9bef-0edb-4bb0-9ec5-ab85cc7000d7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best XGBoost Accuracy: 0.8185\n",
      "Best XGBoost Params: {'classifier__max_depth': 8, 'classifier__learning_rate': 0.01, 'classifier__subsample': 0.8, 'classifier__colsample_bytree': 0.8, 'classifier__min_child_weight': 10, 'classifier__gamma': 1, 'classifier__reg_alpha': 1, 'classifier__reg_lambda': 0.001, 'classifier__n_estimators': 200}\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Info] Number of positive: 3998, number of negative: 3998\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002062 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5808\n",
      "[LightGBM] [Info] Number of data points in the train set: 7996, number of used features: 44\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\baudz\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "[00:16:23] INFO: Trial 1/1 - Accuracy: 0.8150 - Params: {'classifier__boosting_type': 'dart', 'classifier__n_estimators': 600, 'classifier__learning_rate': 0.2, 'classifier__num_leaves': 255, 'classifier__max_depth': 12, 'classifier__feature_fraction': 1.0, 'classifier__bagging_fraction': 0.7, 'classifier__min_child_samples': 5, 'classifier__lambda_l1': 0.01, 'classifier__lambda_l2': 0.01}\n",
      "[I 2025-11-15 00:16:23,245] Trial 0 finished with value: 0.815 and parameters: {'classifier__boosting_type': 'dart', 'classifier__n_estimators': 600, 'classifier__learning_rate': 0.2, 'classifier__num_leaves': 255, 'classifier__max_depth': 12, 'classifier__feature_fraction': 1.0, 'classifier__bagging_fraction': 0.7, 'classifier__min_child_samples': 5, 'classifier__lambda_l1': 0.01, 'classifier__lambda_l2': 0.01}. Best is trial 0 with value: 0.815.\n",
      "[00:16:23] INFO: Optimization finished. Best Accuracy: 0.8150\n",
      "[00:16:23] INFO: Best Parameters: {'classifier__boosting_type': 'dart', 'classifier__n_estimators': 600, 'classifier__learning_rate': 0.2, 'classifier__num_leaves': 255, 'classifier__max_depth': 12, 'classifier__feature_fraction': 1.0, 'classifier__bagging_fraction': 0.7, 'classifier__min_child_samples': 5, 'classifier__lambda_l1': 0.01, 'classifier__lambda_l2': 0.01}\n",
      "[00:16:23] INFO: Fitting the best pipeline on the provided training data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Info] Number of positive: 3998, number of negative: 3998\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001495 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5808\n",
      "[LightGBM] [Info] Number of data points in the train set: 7996, number of used features: 44\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "\n",
      "Best LightGBM Accuracy: 0.815\n",
      "Best LightGBM Params: {'classifier__boosting_type': 'dart', 'classifier__n_estimators': 600, 'classifier__learning_rate': 0.2, 'classifier__num_leaves': 255, 'classifier__max_depth': 12, 'classifier__feature_fraction': 1.0, 'classifier__bagging_fraction': 0.7, 'classifier__min_child_samples': 5, 'classifier__lambda_l1': 0.01, 'classifier__lambda_l2': 0.01}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# --- GRADIENT BOOSTING ---\\nbest_gb_model, best_params_gb, best_score_gb = optimize_optuna(\\n    lambda: pipeline_gradient_boost,\\n    X_train_split,\\n    y_train_split,\\n    X_val_split,\\n    y_val_split,\\n    gradientboost_param_grid,\\n    n_trials=1\\n)\\nprint(\"\\nBest Gradient Boosting Accuracy:\", best_score_gb)\\nprint(\"Best Gradient Boosting Params:\", best_params_gb)\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- XGBOOST ---\n",
    "best_model, best_params_xgb, best_score_xgb = optimize_optuna(\n",
    "    lambda: pipeline_xgb,\n",
    "    X_train_split,\n",
    "    y_train_split,\n",
    "    X_val_split,\n",
    "    y_val_split,\n",
    "    xgboost_param_grid,\n",
    "    n_trials=50\n",
    ")\n",
    "print(\"\\nBest XGBoost Accuracy:\", best_score_xgb)\n",
    "print(\"Best XGBoost Params:\", best_params_xgb)\n",
    "\n",
    "\n",
    "# --- LIGHTGBM ---\n",
    "best_lgbm_model, best_params_lgb, best_score_lgb = optimize_optuna(\n",
    "    lambda: pipeline_lgbm,\n",
    "    X_train_split,\n",
    "    y_train_split,\n",
    "    X_val_split,\n",
    "    y_val_split,\n",
    "    lightgbm_param_grid,\n",
    "    n_trials=50\n",
    ")\n",
    "print(\"\\nBest LightGBM Accuracy:\", best_score_lgb)\n",
    "print(\"Best LightGBM Params:\", best_params_lgb)\n",
    "\n",
    "'''\n",
    "# --- CATBOOST ---\n",
    "best_cat_model, best_params_cat, best_score_cat = optimize_optuna(\n",
    "    lambda: pipeline_catboost,\n",
    "    X_train_split,\n",
    "    y_train_split,\n",
    "    X_val_split,\n",
    "    y_val_split,\n",
    "    catboost_param_grid,\n",
    "    n_trials=1\n",
    ")\n",
    "print(\"\\nBest CatBoost Accuracy:\", best_score_cat)\n",
    "print(\"Best CatBoost Params:\", best_params_cat)\n",
    "'''\n",
    "'''\n",
    "# --- GRADIENT BOOSTING ---\n",
    "best_gb_model, best_params_gb, best_score_gb = optimize_optuna(\n",
    "    lambda: pipeline_gradient_boost,\n",
    "    X_train_split,\n",
    "    y_train_split,\n",
    "    X_val_split,\n",
    "    y_val_split,\n",
    "    gradientboost_param_grid,\n",
    "    n_trials=1\n",
    ")\n",
    "print(\"\\nBest Gradient Boosting Accuracy:\", best_score_gb)\n",
    "print(\"Best Gradient Boosting Params:\", best_params_gb)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5599cc17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Info] Number of positive: 3998, number of negative: 3998\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001869 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5808\n",
      "[LightGBM] [Info] Number of data points in the train set: 7996, number of used features: 44\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# --- Ensure base models are already fitted ---\n",
    "best_model.fit(X_train_split, y_train_split)\n",
    "best_lgbm_model.fit(X_train_split, y_train_split)\n",
    "\n",
    "# --- Create stacking classifier ---\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=[\n",
    "        (\"xgb\", best_model),\n",
    "        (\"lgbm\", best_lgbm_model)\n",
    "        # Add more pre-fitted models if needed\n",
    "        # (\"cat\", best_cat_model)\n",
    "    ],\n",
    "    final_estimator=LogisticRegression(max_iter=5000, random_state=42),\n",
    "    passthrough=True,  # base model predictions included as features\n",
    "    n_jobs=-1,\n",
    "    cv=None  # use full training data to speed things up\n",
    ")\n",
    "\n",
    "# --- Fit stacking model ---\n",
    "stacking_model.fit(X_train_split, y_train_split)\n",
    "\n",
    "# --- Predict and evaluate ---\n",
    "stack_preds = stacking_model.predict(X_val_split)\n",
    "stack_acc = accuracy_score(y_val_split, stack_preds)\n",
    "print(f\"Stacking Ensemble Validation Accuracy: {stack_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5c4188",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = stacking_model\n",
    "\n",
    "\n",
    "from Submission.submit import save_submission\n",
    "import pandas as pd\n",
    "\n",
    "save_submission(X_test_features, final_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
