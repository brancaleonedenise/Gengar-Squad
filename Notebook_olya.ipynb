{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0f8bb2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Local environment detected. Loading data from: Data\n",
      "✓ train.jsonl loaded successfully. Shape: (10000, 5)\n",
      "✓ test.jsonl loaded successfully. Shape: (5000, 4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "962b0114f57d4d9582edc7d38448d360",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating advanced features:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49e0d8d0e3804832ab00817c569df19c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating advanced features:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:\n",
      "(8000, 110) (2000, 110) (8000,) (2000,)\n"
     ]
    }
   ],
   "source": [
    "from main import load_data\n",
    "from Features.features_olya import create_advanced_features_gen2\n",
    "import os\n",
    "import pandas as pd\n",
    "from utils.load_json import load_jsonl\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "test_size=0.2\n",
    "random_state=42\n",
    "train_df, test_df = load_data()\n",
    "\n",
    "# Subset already created\n",
    "fraction_to_use = 1.0\n",
    "train_df_subset = train_df.sample(frac=fraction_to_use, random_state=42).reset_index(drop=True)\n",
    "test_df_subset = test_df.sample(frac=fraction_to_use, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Feature engineering\n",
    "X_train_features = create_advanced_features_gen2(train_df_subset)\n",
    "X_test_features = create_advanced_features_gen2(test_df_subset)\n",
    "\n",
    "\n",
    "# Target\n",
    "y_train = train_df_subset.set_index('battle_id')['player_won']\n",
    "y_test = test_df_subset.set_index('battle_id')\n",
    "\n",
    "# Train/validation split on the subset ONLY\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
    "    X_train_features,  # must match y_train\n",
    "    y_train,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y_train\n",
    ")\n",
    "\n",
    "print(\"Shapes:\")\n",
    "print(X_train_split.shape, X_val_split.shape, y_train_split.shape, y_val_split.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f82e98b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "import numpy as np\n",
    "\n",
    "# --- Expand Pokémon columns ---\n",
    "def expand_seen_pokemon_features(df, prefix='p1_seen_pokemons'):\n",
    "    if prefix not in df.columns:\n",
    "        return df\n",
    "    num_pokemon = len(df[prefix].iloc[0])\n",
    "    expanded = pd.DataFrame(\n",
    "        df[prefix].tolist(),\n",
    "        index=df.index,\n",
    "        columns=[f\"{prefix}_{i}\" for i in range(num_pokemon)]\n",
    "    )\n",
    "    df = df.drop(columns=[prefix])\n",
    "    return pd.concat([df, expanded], axis=1)\n",
    "\n",
    "for col_prefix in ['p1_seen_pokemons', 'p2_seen_pokemons']:\n",
    "    X_train_split = expand_seen_pokemon_features(X_train_split, col_prefix)\n",
    "    X_val_split = expand_seen_pokemon_features(X_val_split, col_prefix)\n",
    "    X_test_features = expand_seen_pokemon_features(X_test_features, col_prefix)\n",
    "\n",
    "# --- Existing feature groups ---\n",
    "lead_embedding_columns = [\n",
    "    'p1_lead_hp','p1_lead_atk','p1_lead_def','p1_lead_spa','p1_lead_spd','p1_lead_spe',\n",
    "    'p2_lead_hp','p2_lead_atk','p2_lead_def','p2_lead_spa','p2_lead_spd','p2_lead_spe',\n",
    "    'p1_team_emb_sum_hp', 'p1_team_emb_sum_atk', 'p1_team_emb_sum_def', 'p1_team_emb_sum_spa',\n",
    "    'p1_team_emb_sum_spd', 'p1_team_emb_sum_spe',\n",
    "    'p1_team_emb_mean_hp', 'p1_team_emb_mean_atk', 'p1_team_emb_mean_def', 'p1_team_emb_mean_spa',\n",
    "    'p1_team_emb_mean_spd', 'p1_team_emb_mean_spe',\n",
    "    'p2_team_emb_sum_hp', 'p2_team_emb_sum_atk', 'p2_team_emb_sum_def', 'p2_team_emb_sum_spa',\n",
    "    'p2_team_emb_sum_spd', 'p2_team_emb_sum_spe',\n",
    "    'p2_team_emb_mean_hp', 'p2_team_emb_mean_atk', 'p2_team_emb_mean_def', 'p2_team_emb_mean_spa',\n",
    "    'p2_team_emb_mean_spd', 'p2_team_emb_mean_spe'\n",
    "]\n",
    "\n",
    "pokemon_columns = [c for c in X_train_split.columns if c.startswith(('p1_seen_pokemons_', 'p2_seen_pokemons_'))]\n",
    "\n",
    "core_features = [\n",
    "    'lead_speed_diff','hp_advantage_seen','mons_revealed_diff','team_status_diff',\n",
    "    'total_damage_dealt','status_turns','lead_type_adv','meta_diff','hp_trend_diff',\n",
    "    'aggression_index','hp_diff_std','momentum_shift_turn'\n",
    "]\n",
    "\n",
    "correlation_features = [\n",
    "    'p1_lead_special_total','p2_lead_special_total','special_total_diff',\n",
    "    'p1_lead_physical_total','p2_lead_physical_total','physical_total_diff',\n",
    "    'atk_def_ratio_p1','atk_def_ratio_p2',\n",
    "    'hp_speed_interaction_lead',\n",
    "    'hp_def_ratio_p1','hp_def_ratio_p2',\n",
    "    'hp_vs_total_stats_p1','hp_vs_total_stats_p2'\n",
    "]\n",
    "\n",
    "# --- Gen3 advanced features ---\n",
    "advanced_features = [\n",
    "    # Existing advanced features\n",
    "    'feat_switch_diff','feat_aggression_diff','comeback_score','damage_ratio_turn25_30',\n",
    "    'stall_ratio','boost_volatility','boost_trend',\n",
    "    'p1_hp_min', 'p1_hp_max', 'p1_hp_mean', 'p1_hp_trend',\n",
    "    'p1_boost_min', 'p1_boost_max', 'p1_boost_mean', 'p1_boost_trend',\n",
    "    'p1_status_total', 'p1_status_ratio',\n",
    "    'p2_hp_min', 'p2_hp_max', 'p2_hp_mean', 'p2_hp_trend',\n",
    "    'p2_boost_min', 'p2_boost_max', 'p2_boost_mean', 'p2_boost_trend',\n",
    "    'p2_status_total', 'p2_status_ratio',\n",
    "    'hp_diff_std', 'hp_diff_range', 'momentum_shift_turn',\n",
    "    'comeback_score', 'early_sustain', 'feat_move_power_diff',\n",
    "    'status_balance',           # <- newly added\n",
    "    'move_diversity_diff',      # <- newly added\n",
    "    'feat_status_diff_inflicted', # <- newly added\n",
    "]\n",
    "\n",
    "\n",
    "advanced_features += correlation_features\n",
    "\n",
    "extra_features = [\n",
    "    'end_boost_diff',\n",
    "    'total_healing_done','first_faint_turn',\n",
    "    'status_setup_diff','total_stats_diff',\n",
    "    'damage_diff_turn10','damage_diff_turn20','damage_diff_turn25','damage_diff_turn30',\n",
    "    'boost_volatility',\n",
    "    'move_power_diff','stats_speed_interaction',\n",
    "    'hp_vs_stats_ratio','lead_total_stats_p1','lead_total_stats_p2',\n",
    "    'atk_hp_ratio_p1','atk_hp_ratio_p2','def_hp_ratio_p1','def_hp_ratio_p2',\n",
    "    'feat_hp_trend_diff','hp_diff_mean',\n",
    "    'hp_diff_last','boost_diff_mean',\n",
    "    'momentum_flips','p1_aggression','p2_aggression'\n",
    "]\n",
    "\n",
    "advanced_features += extra_features\n",
    "\n",
    "\n",
    "# Utility to evaluate and print\n",
    "def evaluate_model(name, model, X_train, y_train, X_val, y_val):\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_val)\n",
    "    acc = accuracy_score(y_val, preds)\n",
    "    print(f\"{name} Validation Accuracy: {acc:.4f}\")\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba17fb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Combine all features you want\n",
    "all_features = [\n",
    "    'lead_speed_diff','hp_advantage_seen','mons_revealed_diff','team_status_diff',\n",
    "    'end_boost_diff','total_damage_dealt','total_healing_done','status_turns',\n",
    "    'first_faint_turn','lead_type_adv','meta_diff','status_setup_diff','total_stats_diff',\n",
    "    'damage_diff_turn10','damage_diff_turn20','damage_diff_turn25','damage_diff_turn30',\n",
    "    'hp_trend_diff','feat_switch_diff','feat_aggression_diff','hp_diff_std','hp_diff_range',\n",
    "    'momentum_shift_turn','comeback_score','early_sustain','status_balance','boost_volatility',\n",
    "    'boost_trend','move_power_diff','move_diversity_diff','stall_ratio','aggression_index',\n",
    "    'stats_speed_interaction','hp_vs_stats_ratio','damage_ratio_turn25_30','damage_ratio_turn20_25',\n",
    "    'damage_ratio_turn10_20','damage_ratio_turn10_30','p1_lead_special_total','p2_lead_special_total',\n",
    "    'special_total_diff','p1_lead_physical_total','p2_lead_physical_total','physical_total_diff',\n",
    "    'atk_def_ratio_p1','atk_def_ratio_p2','hp_speed_interaction_lead','hp_def_ratio_p1','hp_def_ratio_p2',\n",
    "    'hp_vs_total_stats_p1','hp_vs_total_stats_p2','lead_total_stats_p1','lead_total_stats_p2',\n",
    "    'atk_hp_ratio_p1','atk_hp_ratio_p2','def_hp_ratio_p1','def_hp_ratio_p2','feat_hp_trend_diff',\n",
    "    'feat_status_diff_inflicted','p1_hp_mean','p2_hp_mean','hp_diff_mean','hp_diff_last','p1_boost_mean',\n",
    "    'p2_boost_mean','boost_diff_mean','p1_status_total','p2_status_total','momentum_flips',\n",
    "    'p1_aggression','p2_aggression','aggression_diff'\n",
    "] + ['p1_lead_hp','p1_lead_atk','p1_lead_def','p1_lead_spa','p1_lead_spd','p1_lead_spe',\n",
    "     'p2_lead_hp','p2_lead_atk','p2_lead_def','p2_lead_spa','p2_lead_spd','p2_lead_spe']\n",
    "\n",
    "# Keep only the features present in the dataframe\n",
    "existing_features = [f for f in all_features if f in X_train_split.columns]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24edb395",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_features = existing_features\n",
    "\n",
    "preprocessor_log = ColumnTransformer(\n",
    "    [('num', StandardScaler(), log_features)]\n",
    ")\n",
    "\n",
    "pipeline_log = Pipeline([\n",
    "    ('preprocessor', preprocessor_log),\n",
    "    ('classifier', LogisticRegression(max_iter=8000, solver='saga', random_state=42))\n",
    "])\n",
    "\n",
    "param_dist_log = {\n",
    "    'classifier__C': [10,50,100,150,175,200],\n",
    "    'classifier__penalty': ['l1', 'l2'],\n",
    "    'classifier__class_weight': [None, 'balanced']\n",
    "}\n",
    "\n",
    "search_log = RandomizedSearchCV(\n",
    "    pipeline_log, param_distributions=param_dist_log,\n",
    "    n_iter=15, scoring='accuracy', n_jobs=-1, cv=5, random_state=42\n",
    ")\n",
    "search_log.fit(X_train_split, y_train_split)\n",
    "\n",
    "log_best = search_log.best_estimator_\n",
    "log_acc = accuracy_score(y_val_split, log_best.predict(X_val_split))\n",
    "print(\"Logistic Regression\")\n",
    "print(\"Best Params:\", search_log.best_params_)\n",
    "print(f\"Validation Accuracy: {log_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85aafa4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importances (LR):\n",
      "                        feature  importance\n",
      "2           mons_revealed_diff   29.924924\n",
      "66             p1_status_total   26.829265\n",
      "3             team_status_diff   15.246327\n",
      "58  feat_status_diff_inflicted   15.246327\n",
      "25              status_balance   15.246327\n",
      "..                         ...         ...\n",
      "31            aggression_index    0.000000\n",
      "40          special_total_diff    0.000000\n",
      "41      p1_lead_physical_total    0.000000\n",
      "43         physical_total_diff    0.000000\n",
      "42      p2_lead_physical_total    0.000000\n",
      "\n",
      "[84 rows x 2 columns]\n",
      "\n",
      "Keeping 58 features out of 84\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# --- Extract numeric features from preprocessor\n",
    "num_features = log_best.named_steps['preprocessor'].transformers_[0][2]\n",
    "\n",
    "# --- Get absolute coefficient values as importances\n",
    "coefs = log_best.named_steps['classifier'].coef_[0]\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'feature': num_features,\n",
    "    'importance': np.abs(coefs)\n",
    "}).sort_values(by='importance', ascending=False)\n",
    "\n",
    "print(\"Feature importances (LR):\\n\", feature_importance_df)\n",
    "\n",
    "# --- Keep features above threshold (e.g., 0.01)\n",
    "threshold = 0.01\n",
    "selected_features = feature_importance_df[feature_importance_df['importance'] > threshold]['feature'].tolist()\n",
    "print(f\"\\nKeeping {len(selected_features)} features out of {len(num_features)}\")\n",
    "\n",
    "# --- Build new pipeline with selected features\n",
    "preprocessor_selected = ColumnTransformer([('num', StandardScaler(), selected_features)])\n",
    "pipeline_selected = Pipeline([\n",
    "    ('preprocessor', preprocessor_selected),\n",
    "    ('classifier', LogisticRegression(max_iter=8000, solver='saga', random_state=42))\n",
    "])\n",
    "\n",
    "param_dist_selected = {\n",
    "    'classifier__C': [10,50,100,150,175],\n",
    "    'classifier__penalty': ['l1', 'l2'],\n",
    "    'classifier__class_weight': [None, 'balanced']\n",
    "}\n",
    "\n",
    "search_selected = RandomizedSearchCV(\n",
    "    pipeline_selected,\n",
    "    param_distributions=param_dist_selected,\n",
    "    n_iter=15,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    cv=5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "search_selected.fit(X_train_split, y_train_split)\n",
    "\n",
    "# --- Evaluate\n",
    "log_best_selected = search_selected.best_estimator_\n",
    "log_acc_selected = accuracy_score(y_val_split, log_best_selected.predict(X_val_split))\n",
    "\n",
    "print(\"Logistic Regression (selected features)\")\n",
    "print(\"Best Params:\", search_selected.best_params_)\n",
    "print(f\"Validation Accuracy: {log_acc_selected:.4f}\")\n",
    "print(f\"Accuracy change: {log_acc_selected - log_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e1c138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Best Params: {'classifier__n_estimators': 300, 'classifier__min_samples_split': 2, 'classifier__min_samples_leaf': 1, 'classifier__max_features': 'sqrt', 'classifier__max_depth': None}\n",
      "Validation Accuracy: 0.8350\n"
     ]
    }
   ],
   "source": [
    "rf_features = existing_features\n",
    "\n",
    "preprocessor_rf = ColumnTransformer([('num', StandardScaler(), rf_features)])\n",
    "\n",
    "pipeline_rf = Pipeline([\n",
    "    ('preprocessor', preprocessor_rf),\n",
    "    ('classifier', RandomForestClassifier(random_state=42, n_jobs=-1))\n",
    "])\n",
    "\n",
    "param_dist_rf = {\n",
    "    'classifier__n_estimators': [200, 300, 400],\n",
    "    'classifier__max_depth': [10, 20, None],\n",
    "    'classifier__min_samples_split': [2, 5],\n",
    "    'classifier__min_samples_leaf': [1, 2, 4],\n",
    "    'classifier__max_features': ['sqrt', 0.5, 0.7]\n",
    "}\n",
    "\n",
    "search_rf = RandomizedSearchCV(\n",
    "    pipeline_rf, param_distributions=param_dist_rf,\n",
    "    n_iter=15, scoring='accuracy', n_jobs=-1, cv=5, random_state=42\n",
    ")\n",
    "search_rf.fit(X_train_split, y_train_split)\n",
    "\n",
    "rf_best = search_rf.best_estimator_\n",
    "rf_acc = accuracy_score(y_val_split, rf_best.predict(X_val_split))\n",
    "print(\"Random Forest\")\n",
    "print(\"Best Params:\", search_rf.best_params_)\n",
    "print(f\"Validation Accuracy: {rf_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d60fdc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importances:\n",
      "                       feature  importance\n",
      "1           hp_advantage_seen    0.262347\n",
      "3            team_status_diff    0.195363\n",
      "2          mons_revealed_diff    0.093715\n",
      "8               hp_trend_diff    0.074596\n",
      "9            aggression_index    0.064927\n",
      "15     damage_ratio_turn25_30    0.042100\n",
      "4          total_damage_dealt    0.040265\n",
      "10                hp_diff_std    0.026228\n",
      "11        momentum_shift_turn    0.025986\n",
      "5                status_turns    0.025930\n",
      "14             comeback_score    0.024937\n",
      "17           boost_volatility    0.016876\n",
      "0             lead_speed_diff    0.011748\n",
      "24        physical_total_diff    0.009156\n",
      "21         special_total_diff    0.008902\n",
      "18                boost_trend    0.006951\n",
      "43                p2_lead_spe    0.004982\n",
      "16                stall_ratio    0.004352\n",
      "37                p1_lead_spe    0.004030\n",
      "27  hp_speed_interaction_lead    0.003956\n",
      "28            hp_def_ratio_p1    0.003612\n",
      "29            hp_def_ratio_p2    0.003297\n",
      "25           atk_def_ratio_p1    0.003243\n",
      "31       hp_vs_total_stats_p2    0.003209\n",
      "26           atk_def_ratio_p2    0.003204\n",
      "36                p1_lead_spd    0.003200\n",
      "41                p2_lead_spa    0.003138\n",
      "35                p1_lead_spa    0.002984\n",
      "30       hp_vs_total_stats_p1    0.002811\n",
      "38                 p2_lead_hp    0.002692\n",
      "42                p2_lead_spd    0.002647\n",
      "19      p1_lead_special_total    0.002628\n",
      "20      p2_lead_special_total    0.002620\n",
      "33                p1_lead_atk    0.002188\n",
      "32                 p1_lead_hp    0.002170\n",
      "22     p1_lead_physical_total    0.002111\n",
      "23     p2_lead_physical_total    0.001840\n",
      "39                p2_lead_atk    0.001821\n",
      "34                p1_lead_def    0.001661\n",
      "40                p2_lead_def    0.001576\n",
      "6               lead_type_adv    0.000000\n",
      "13       feat_aggression_diff    0.000000\n",
      "7                   meta_diff    0.000000\n",
      "12           feat_switch_diff    0.000000\n",
      "\n",
      "Keeping 13 features out of 44\n",
      "Random Forest (selected features)\n",
      "Best Params: {'classifier__n_estimators': 300, 'classifier__min_samples_split': 2, 'classifier__min_samples_leaf': 1, 'classifier__max_features': 'sqrt', 'classifier__max_depth': 20}\n",
      "Validation Accuracy: 0.8270\n",
      "\n",
      "Accuracy change: -0.0035\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 1️⃣ Extract numeric feature names from ColumnTransformer\n",
    "num_features = rf_best.named_steps['preprocessor'].transformers_[0][2]\n",
    "\n",
    "# 2️⃣ Get feature importances\n",
    "rf_model = rf_best.named_steps['classifier']\n",
    "importances = rf_model.feature_importances_\n",
    "\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'feature': num_features,\n",
    "    'importance': importances\n",
    "}).sort_values(by='importance', ascending=False)\n",
    "\n",
    "print(\"Feature importances:\\n\", feature_importance_df)\n",
    "\n",
    "# 3️⃣ Select only features above a threshold (e.g., 1% importance)\n",
    "threshold = 0.01  # adjust based on how aggressive you want to be\n",
    "selected_features = feature_importance_df[feature_importance_df['importance'] > threshold]['feature'].tolist()\n",
    "\n",
    "print(f\"\\nKeeping {len(selected_features)} features out of {len(num_features)}\")\n",
    "\n",
    "# 4️⃣ Build new preprocessor and pipeline with selected features\n",
    "preprocessor_selected = ColumnTransformer([('num', StandardScaler(), selected_features)])\n",
    "\n",
    "pipeline_selected = Pipeline([\n",
    "    ('preprocessor', preprocessor_selected),\n",
    "    ('classifier', RandomForestClassifier(random_state=42, n_jobs=-1))\n",
    "])\n",
    "\n",
    "param_dist_selected = {\n",
    "    'classifier__n_estimators': [200, 300, 400],\n",
    "    'classifier__max_depth': [10, 20, None],\n",
    "    'classifier__min_samples_split': [2, 5],\n",
    "    'classifier__min_samples_leaf': [1, 2, 4],\n",
    "    'classifier__max_features': ['sqrt', 0.5, 0.7]\n",
    "}\n",
    "\n",
    "# 5️⃣ Randomized Search on selected features\n",
    "search_selected = RandomizedSearchCV(\n",
    "    pipeline_selected,\n",
    "    param_distributions=param_dist_selected,\n",
    "    n_iter=15,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    cv=5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "search_selected.fit(X_train_split, y_train_split)\n",
    "\n",
    "# 6️⃣ Evaluate\n",
    "rf_best_selected = search_selected.best_estimator_\n",
    "rf_acc_selected = accuracy_score(y_val_split, rf_best_selected.predict(X_val_split))\n",
    "\n",
    "print(\"Random Forest (selected features)\")\n",
    "print(\"Best Params:\", search_selected.best_params_)\n",
    "print(f\"Validation Accuracy: {rf_acc_selected:.4f}\")\n",
    "\n",
    "# 7️⃣ Compare\n",
    "print(f\"\\nAccuracy change: {rf_acc_selected - rf_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fa86e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c137eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045f282d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-15 18:03:40,510] A new study created in memory with name: no-name-1040c3e1-74a1-45ff-a367-90e3feff92c4\n",
      "[I 2025-11-15 18:03:41,678] Trial 0 finished with value: 0.82 and parameters: {'max_depth': 4, 'learning_rate': 0.20591800753925607, 'subsample': 0.7963113978206501, 'colsample_bytree': 0.7117088662531388}. Best is trial 0 with value: 0.82.\n",
      "[I 2025-11-15 18:03:43,155] Trial 1 finished with value: 0.833 and parameters: {'max_depth': 5, 'learning_rate': 0.06399734459170783, 'subsample': 0.8429877457601078, 'colsample_bytree': 0.705090901396117}. Best is trial 1 with value: 0.833.\n",
      "[I 2025-11-15 18:03:44,231] Trial 2 finished with value: 0.8295 and parameters: {'max_depth': 3, 'learning_rate': 0.08260070486689643, 'subsample': 0.9148641683315223, 'colsample_bytree': 0.809509968960166}. Best is trial 1 with value: 0.833.\n",
      "[I 2025-11-15 18:03:46,035] Trial 3 finished with value: 0.83 and parameters: {'max_depth': 6, 'learning_rate': 0.11329452138859876, 'subsample': 0.9394216522364026, 'colsample_bytree': 0.8817626570077511}. Best is trial 1 with value: 0.833.\n",
      "[I 2025-11-15 18:03:48,283] Trial 4 finished with value: 0.82 and parameters: {'max_depth': 7, 'learning_rate': 0.15406025884882565, 'subsample': 0.8028521462748087, 'colsample_bytree': 0.8040906862155871}. Best is trial 1 with value: 0.833.\n",
      "[I 2025-11-15 18:03:52,661] Trial 5 finished with value: 0.8265 and parameters: {'max_depth': 7, 'learning_rate': 0.04469311572009812, 'subsample': 0.8407307476574053, 'colsample_bytree': 0.7713375644313774}. Best is trial 1 with value: 0.833.\n",
      "[I 2025-11-15 18:03:57,402] Trial 6 finished with value: 0.8315 and parameters: {'max_depth': 3, 'learning_rate': 0.03010310590806324, 'subsample': 0.6580881611729675, 'colsample_bytree': 0.6141703914181329}. Best is trial 1 with value: 0.833.\n",
      "[I 2025-11-15 18:03:59,171] Trial 7 finished with value: 0.83 and parameters: {'max_depth': 6, 'learning_rate': 0.07973184842618572, 'subsample': 0.7058065461612596, 'colsample_bytree': 0.6315900481983668}. Best is trial 1 with value: 0.833.\n",
      "[I 2025-11-15 18:04:00,255] Trial 8 finished with value: 0.8345 and parameters: {'max_depth': 3, 'learning_rate': 0.05152573592312359, 'subsample': 0.8830783471771546, 'colsample_bytree': 0.7522129669751393}. Best is trial 8 with value: 0.8345.\n",
      "[I 2025-11-15 18:04:02,534] Trial 9 finished with value: 0.8315 and parameters: {'max_depth': 7, 'learning_rate': 0.10093658726382283, 'subsample': 0.8736125287362875, 'colsample_bytree': 0.7053403622269256}. Best is trial 8 with value: 0.8345.\n",
      "[I 2025-11-15 18:04:03,919] Trial 10 finished with value: 0.833 and parameters: {'max_depth': 4, 'learning_rate': 0.012053765886091859, 'subsample': 0.9733653164877402, 'colsample_bytree': 0.9900453693856925}. Best is trial 8 with value: 0.8345.\n",
      "[I 2025-11-15 18:04:05,404] Trial 11 finished with value: 0.836 and parameters: {'max_depth': 5, 'learning_rate': 0.02819768775202474, 'subsample': 0.7627724921885286, 'colsample_bytree': 0.7077983088402628}. Best is trial 11 with value: 0.836.\n",
      "[I 2025-11-15 18:04:11,928] Trial 12 finished with value: 0.833 and parameters: {'max_depth': 4, 'learning_rate': 0.023116159023994397, 'subsample': 0.7384977526127091, 'colsample_bytree': 0.8704968905842306}. Best is trial 11 with value: 0.836.\n",
      "[I 2025-11-15 18:04:13,377] Trial 13 finished with value: 0.8355 and parameters: {'max_depth': 5, 'learning_rate': 0.01976513241612449, 'subsample': 0.7543303033724621, 'colsample_bytree': 0.7571653145896572}. Best is trial 11 with value: 0.836.\n",
      "[I 2025-11-15 18:04:14,738] Trial 14 finished with value: 0.8335 and parameters: {'max_depth': 5, 'learning_rate': 0.012250935572181392, 'subsample': 0.6022184604265782, 'colsample_bytree': 0.6562901926701588}. Best is trial 11 with value: 0.836.\n",
      "[I 2025-11-15 18:04:16,627] Trial 15 finished with value: 0.8385 and parameters: {'max_depth': 6, 'learning_rate': 0.020650099702276964, 'subsample': 0.745596894806507, 'colsample_bytree': 0.8522145216606439}. Best is trial 15 with value: 0.8385.\n",
      "[I 2025-11-15 18:04:18,726] Trial 16 finished with value: 0.836 and parameters: {'max_depth': 6, 'learning_rate': 0.027899452480868833, 'subsample': 0.6824703049173444, 'colsample_bytree': 0.8752125193521191}. Best is trial 15 with value: 0.8385.\n",
      "[I 2025-11-15 18:04:23,470] Trial 17 finished with value: 0.8345 and parameters: {'max_depth': 6, 'learning_rate': 0.016927436752648507, 'subsample': 0.7762055141504699, 'colsample_bytree': 0.9909001187877358}. Best is trial 15 with value: 0.8385.\n",
      "[I 2025-11-15 18:04:28,598] Trial 18 finished with value: 0.835 and parameters: {'max_depth': 6, 'learning_rate': 0.034814592444252716, 'subsample': 0.6335695275371316, 'colsample_bytree': 0.8541577933039315}. Best is trial 15 with value: 0.8385.\n",
      "[I 2025-11-15 18:04:30,181] Trial 19 finished with value: 0.832 and parameters: {'max_depth': 5, 'learning_rate': 0.016393318137309622, 'subsample': 0.7193878608220697, 'colsample_bytree': 0.938663337949279}. Best is trial 15 with value: 0.8385.\n",
      "[I 2025-11-15 18:04:31,718] Trial 20 finished with value: 0.829 and parameters: {'max_depth': 5, 'learning_rate': 0.010160707214899435, 'subsample': 0.8165781644074064, 'colsample_bytree': 0.917163844251942}. Best is trial 15 with value: 0.8385.\n",
      "[I 2025-11-15 18:04:33,494] Trial 21 finished with value: 0.84 and parameters: {'max_depth': 6, 'learning_rate': 0.0289247349399114, 'subsample': 0.6964628680100533, 'colsample_bytree': 0.8374889309046434}. Best is trial 21 with value: 0.84.\n",
      "[I 2025-11-15 18:04:35,278] Trial 22 finished with value: 0.8335 and parameters: {'max_depth': 6, 'learning_rate': 0.035866393392763776, 'subsample': 0.6977652569848111, 'colsample_bytree': 0.8286332234576871}. Best is trial 21 with value: 0.84.\n",
      "[I 2025-11-15 18:04:43,668] Trial 23 finished with value: 0.8375 and parameters: {'max_depth': 7, 'learning_rate': 0.023569179121789692, 'subsample': 0.7491950709288491, 'colsample_bytree': 0.9186740883236322}. Best is trial 21 with value: 0.84.\n",
      "[I 2025-11-15 18:04:46,138] Trial 24 finished with value: 0.836 and parameters: {'max_depth': 7, 'learning_rate': 0.020400327378212367, 'subsample': 0.6547755480618421, 'colsample_bytree': 0.9253709438055815}. Best is trial 21 with value: 0.84.\n",
      "[I 2025-11-15 18:04:48,498] Trial 25 finished with value: 0.8335 and parameters: {'max_depth': 7, 'learning_rate': 0.0403006192422486, 'subsample': 0.7319323701740796, 'colsample_bytree': 0.913922956618701}. Best is trial 21 with value: 0.84.\n",
      "[I 2025-11-15 18:04:50,329] Trial 26 finished with value: 0.8375 and parameters: {'max_depth': 6, 'learning_rate': 0.023483431256466295, 'subsample': 0.6710721250458224, 'colsample_bytree': 0.8458305783006519}. Best is trial 21 with value: 0.84.\n",
      "[I 2025-11-15 18:04:57,216] Trial 27 finished with value: 0.8085 and parameters: {'max_depth': 7, 'learning_rate': 0.29939172027075095, 'subsample': 0.6282091814866672, 'colsample_bytree': 0.9485485225046192}. Best is trial 21 with value: 0.84.\n",
      "[I 2025-11-15 18:05:00,055] Trial 28 finished with value: 0.8325 and parameters: {'max_depth': 6, 'learning_rate': 0.014703490527932966, 'subsample': 0.7139953499608731, 'colsample_bytree': 0.8956106112645418}. Best is trial 21 with value: 0.84.\n",
      "[I 2025-11-15 18:05:03,062] Trial 29 finished with value: 0.832 and parameters: {'max_depth': 7, 'learning_rate': 0.045594632325238624, 'subsample': 0.7801436500583473, 'colsample_bytree': 0.9693651829508356}. Best is trial 21 with value: 0.84.\n",
      "[I 2025-11-15 18:05:05,236] Trial 30 finished with value: 0.8325 and parameters: {'max_depth': 6, 'learning_rate': 0.0260458533865875, 'subsample': 0.7412119965977646, 'colsample_bytree': 0.7834512209533223}. Best is trial 21 with value: 0.84.\n",
      "[I 2025-11-15 18:05:09,533] Trial 31 finished with value: 0.8355 and parameters: {'max_depth': 6, 'learning_rate': 0.021648446251057052, 'subsample': 0.6842900767036335, 'colsample_bytree': 0.8395690055343005}. Best is trial 21 with value: 0.84.\n",
      "[I 2025-11-15 18:05:15,410] Trial 32 finished with value: 0.83 and parameters: {'max_depth': 6, 'learning_rate': 0.058054985021845106, 'subsample': 0.6670268759582547, 'colsample_bytree': 0.8497529731031449}. Best is trial 21 with value: 0.84.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-15 18:05:18,411] Trial 33 finished with value: 0.8385 and parameters: {'max_depth': 7, 'learning_rate': 0.01822834684910135, 'subsample': 0.7892078213369778, 'colsample_bytree': 0.8241649308875245}. Best is trial 21 with value: 0.84.\n",
      "[I 2025-11-15 18:05:21,202] Trial 34 finished with value: 0.8345 and parameters: {'max_depth': 7, 'learning_rate': 0.014887936148241007, 'subsample': 0.8151710637023682, 'colsample_bytree': 0.8109633746603037}. Best is trial 21 with value: 0.84.\n",
      "[I 2025-11-15 18:05:29,874] Trial 35 finished with value: 0.837 and parameters: {'max_depth': 7, 'learning_rate': 0.018383287559310076, 'subsample': 0.7887893158592039, 'colsample_bytree': 0.8904479797874435}. Best is trial 21 with value: 0.84.\n",
      "[I 2025-11-15 18:05:32,400] Trial 36 finished with value: 0.8365 and parameters: {'max_depth': 7, 'learning_rate': 0.03306796311363276, 'subsample': 0.8246649379917339, 'colsample_bytree': 0.7434808983336767}. Best is trial 21 with value: 0.84.\n",
      "[I 2025-11-15 18:05:35,264] Trial 37 finished with value: 0.833 and parameters: {'max_depth': 7, 'learning_rate': 0.013693200482404766, 'subsample': 0.7621704518235647, 'colsample_bytree': 0.7920937851759499}. Best is trial 21 with value: 0.84.\n",
      "[I 2025-11-15 18:05:38,067] Trial 38 finished with value: 0.8315 and parameters: {'max_depth': 7, 'learning_rate': 0.010148413654922884, 'subsample': 0.8723148298006882, 'colsample_bytree': 0.8289186528062918}. Best is trial 21 with value: 0.84.\n",
      "[I 2025-11-15 18:05:45,132] Trial 39 finished with value: 0.828 and parameters: {'max_depth': 6, 'learning_rate': 0.07340817929360996, 'subsample': 0.7870039198660818, 'colsample_bytree': 0.8136820449679766}. Best is trial 21 with value: 0.84.\n",
      "C:\\Users\\Olha Biziura\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [18:05:45] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best XGBoost Accuracy: 0.84\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# --- 1. Define Top 50 features (from your previous list) ---\n",
    "top50_features = [\n",
    "    'lead_speed_diff', 'total_stats_diff', 'p1_lead_hp', 'p2_lead_hp', 'p1_lead_atk', 'p2_lead_atk',\n",
    "    'p1_lead_def', 'p2_lead_def', 'p1_lead_spa', 'p2_lead_spa', 'p1_lead_spd', 'p2_lead_spd',\n",
    "    'p1_lead_spe', 'p2_lead_spe', 'hp_advantage_seen', 'mons_revealed_diff', 'team_status_diff',\n",
    "    'feat_hp_trend_diff', 'total_damage_dealt', 'damage_diff_turn10', 'damage_diff_turn25', 'damage_diff_turn30',\n",
    "    'hp_diff_std', 'hp_diff_range', 'comeback_score', 'early_sustain', 'status_turns', 'first_faint_turn',\n",
    "    'status_balance', 'boost_volatility', 'boost_trend', 'status_setup_diff', 'feat_status_diff_inflicted',\n",
    "    'feat_switch_diff', 'feat_aggression_diff', 'move_power_diff', 'move_diversity_diff', 'stall_ratio',\n",
    "    'aggression_index', 'stats_speed_interaction', 'hp_vs_stats_ratio', 'damage_ratio_turn25_30',\n",
    "    'damage_ratio_turn20_25', 'damage_ratio_turn10_20', 'damage_ratio_turn10_30', 'special_total_diff',\n",
    "    'physical_total_diff', 'hp_speed_interaction_lead', 'atk_def_ratio_p1', 'atk_def_ratio_p2'\n",
    "] \n",
    "top50_features+= existing_features\n",
    "# --- 2. Scale numeric features ---\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_split[top50_features])\n",
    "X_val_scaled = scaler.transform(X_val_split[top50_features])\n",
    "\n",
    "# --- 3. Optuna objective for XGBoost ---\n",
    "def xgb_objective(trial):\n",
    "    params = {\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 7),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'n_estimators': 300,\n",
    "        'random_state': 42,\n",
    "        'eval_metric': 'logloss'\n",
    "    }\n",
    "\n",
    "    model = XGBClassifier(**params)\n",
    "    model.fit(\n",
    "        X_train_scaled, y_train_split,\n",
    "        eval_set=[(X_val_scaled, y_val_split)],\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    val_pred = model.predict(X_val_scaled)\n",
    "    return accuracy_score(y_val_split, val_pred)\n",
    "\n",
    "# --- 4. Run Optuna study ---\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(xgb_objective, n_trials=40)\n",
    "\n",
    "# --- 5. Best XGBoost model ---\n",
    "best_xgb_params = study.best_params\n",
    "best_xgb = XGBClassifier(**best_xgb_params, n_estimators=300, \n",
    "                         random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "best_xgb.fit(X_train_scaled, y_train_split,\n",
    "             eval_set=[(X_val_scaled, y_val_split)],\n",
    "             verbose=False)\n",
    "\n",
    "# --- 6. Evaluate ---\n",
    "print(\"Best XGBoost Accuracy:\", accuracy_score(y_val_split, best_xgb.predict(X_val_scaled)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd4cefe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840382fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Info] Number of positive: 4000, number of negative: 4000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002901 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6544\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 70\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Info] Number of positive: 4000, number of negative: 4000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002013 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6544\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 70\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "LightGBM\n",
      "Best Params: {'classifier__num_leaves': 31, 'classifier__n_estimators': 300, 'classifier__max_depth': -1, 'classifier__learning_rate': 0.1, 'classifier__feature_fraction': 0.8, 'classifier__bagging_fraction': 1.0}\n",
      "Validation Accuracy: 0.8320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Olha Biziura\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# --- define features ---\n",
    "lgb_features = existing_features\n",
    "\n",
    "# --- preprocessing ---\n",
    "preprocessor_lgb = ColumnTransformer([\n",
    "    ('num', StandardScaler(), lgb_features)\n",
    "])\n",
    "\n",
    "# --- base model ---\n",
    "lgb_model = LGBMClassifier(\n",
    "    boosting_type='dart',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# --- pipeline for hyperparameter search ---\n",
    "pipeline_lgb = Pipeline([\n",
    "    ('preprocessor', preprocessor_lgb),\n",
    "    ('classifier', lgb_model)\n",
    "])\n",
    "\n",
    "# --- hyperparameter space ---\n",
    "param_dist_lgb = {\n",
    "    'classifier__num_leaves': [31, 63, 127],\n",
    "    'classifier__learning_rate': [0.03, 0.05, 0.1],\n",
    "    'classifier__n_estimators': [300, 500],\n",
    "    'classifier__max_depth': [10, -1],\n",
    "    'classifier__feature_fraction': [0.8, 0.9, 1.0],\n",
    "    'classifier__bagging_fraction': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# --- randomized search ---\n",
    "search_lgb = RandomizedSearchCV(\n",
    "    pipeline_lgb,\n",
    "    param_distributions=param_dist_lgb,\n",
    "    n_iter=20,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    cv=5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# --- fit search ---\n",
    "search_lgb.fit(X_train_split, y_train_split)\n",
    "\n",
    "# --- extract best preprocessor and model ---\n",
    "best_preprocessor = search_lgb.best_estimator_.named_steps['preprocessor']\n",
    "best_model_params = search_lgb.best_params_\n",
    "best_model = LGBMClassifier(\n",
    "    boosting_type='dart',\n",
    "    random_state=42,\n",
    "    n_estimators=best_model_params['classifier__n_estimators'],\n",
    "    num_leaves=best_model_params['classifier__num_leaves'],\n",
    "    learning_rate=best_model_params['classifier__learning_rate'],\n",
    "    max_depth=best_model_params['classifier__max_depth'],\n",
    "    feature_fraction=best_model_params['classifier__feature_fraction'],\n",
    "    bagging_fraction=best_model_params['classifier__bagging_fraction']\n",
    ")\n",
    "\n",
    "# --- preprocess data manually ---\n",
    "X_train_scaled = best_preprocessor.transform(X_train_split)\n",
    "X_val_scaled = best_preprocessor.transform(X_val_split)\n",
    "\n",
    "# --- train with early stopping ---\n",
    "best_model.fit(\n",
    "    X_train_scaled, y_train_split,\n",
    "    eval_set=[(X_val_scaled, y_val_split)],\n",
    "    eval_metric='logloss',\n",
    ")\n",
    "\n",
    "# --- evaluate ---\n",
    "lgb_preds = best_model.predict(X_val_scaled)\n",
    "lgb_acc = accuracy_score(y_val_split, lgb_preds)\n",
    "print(\"LightGBM\")\n",
    "print(\"Best Params:\", best_model_params)\n",
    "print(f\"Validation Accuracy: {lgb_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7048fc11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0490e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatBoost Best Accuracy: 0.8355\n",
      "Best Params: {'learning_rate': 0.03, 'l2_leaf_reg': 3, 'iterations': 800, 'depth': 8}\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# --- 1. Define all features for CatBoost ---\n",
    "cat_features = existing_features\n",
    "\n",
    "# Fill NaNs with 0 to prevent fold mismatch issues\n",
    "X_train_cat = X_train_split[cat_features].fillna(0)\n",
    "X_val_cat   = X_val_split[cat_features].fillna(0)\n",
    "\n",
    "# --- 2. CatBoost model ---\n",
    "cat_model = CatBoostClassifier(\n",
    "    verbose=0,\n",
    "    random_state=42,\n",
    "    task_type='CPU',\n",
    "    early_stopping_rounds=20\n",
    ")\n",
    "\n",
    "# --- 3. Optional hyperparameter search ---\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_dist_cat = {\n",
    "    'depth': [6, 8, 10],\n",
    "    'learning_rate': [0.03, 0.05, 0.1],\n",
    "    'iterations': [300, 500, 800],\n",
    "    'l2_leaf_reg': [3, 5, 7]\n",
    "}\n",
    "\n",
    "search_cat = RandomizedSearchCV(\n",
    "    cat_model,\n",
    "    param_distributions=param_dist_cat,\n",
    "    n_iter=20,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    cv=5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# --- 4. Fit search, pass eval_set directly ---\n",
    "search_cat.fit(\n",
    "    X_train_cat, y_train_split,\n",
    "    eval_set=(X_val_cat, y_val_split)\n",
    ")\n",
    "\n",
    "cat_best = search_cat.best_estimator_\n",
    "cat_acc = accuracy_score(y_val_split, cat_best.predict(X_val_cat))\n",
    "print(\"CatBoost Best Accuracy:\", cat_acc)\n",
    "print(\"Best Params:\", search_cat.best_params_)\n",
    "\n",
    "# --- 5. Ensemble with your other models ---\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28be70e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2785713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Ensemble with your other models ---\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "ensemble = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('xgb', best_xgb),\n",
    "        ('lgb', best_model),\n",
    "        ('cat', cat_best),\n",
    "        ('rf', rf_best),\n",
    "        ('log', log_best)\n",
    "    ],\n",
    "    voting='soft',\n",
    "    weights=[3, 1, 3, 1, 3]\n",
    ")\n",
    "\n",
    "# --- 6. Fit ensemble ---\n",
    "ensemble.fit(X_train_cat, y_train_split)\n",
    "y_pred_ens = ensemble.predict(X_val_cat)\n",
    "ens_acc = accuracy_score(y_val_split, y_pred_ens)\n",
    "print(f\"\\nEnsemble Validation Accuracy: {ens_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9874e93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 8. Predict ---\n",
    "test_pred = ensemble.predict(X_test_features[existing_features])\n",
    "\n",
    "# Convert to int if needed\n",
    "test_pred_int = test_pred.astype(int)\n",
    "\n",
    "# --- 9. Create submission DataFrame ---\n",
    "submission = pd.DataFrame({\n",
    "    'battle_id': test_df_subset['battle_id'],\n",
    "    'player_won': test_pred_int\n",
    "})\n",
    "\n",
    "# --- 10. Save to CSV ---\n",
    "submission.to_csv('submission_olya.csv', index=False)\n",
    "print(\"Submission saved to submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28919a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = ensemble\n",
    "\n",
    "from Submission.submit import save_submission\n",
    "\n",
    "save_submission(X_test_features[existing_features], final_model, name = 'olya_model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
