{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****Caricamento test.jsonl e train.jsonl****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T11:24:40.450455Z",
     "iopub.status.busy": "2025-11-13T11:24:40.450116Z",
     "iopub.status.idle": "2025-11-13T11:24:40.458280Z",
     "shell.execute_reply": "2025-11-13T11:24:40.456728Z",
     "shell.execute_reply.started": "2025-11-13T11:24:40.450432Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import warnings\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# --- 1. Import di Scikit-learn ---\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression   # <-- Il tuo modello\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, RobustScaler # <-- StandardScaler Ã¨ incluso\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T11:24:40.461657Z",
     "iopub.status.busy": "2025-11-13T11:24:40.460535Z",
     "iopub.status.idle": "2025-11-13T11:24:51.333892Z",
     "shell.execute_reply": "2025-11-13T11:24:51.332481Z",
     "shell.execute_reply.started": "2025-11-13T11:24:40.461615Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    train_df = pd.read_json('/kaggle/input/fds-pokemon-battles-prediction-2025/train.jsonl', lines=True)\n",
    "    test_df = pd.read_json('/kaggle/input/fds-pokemon-battles-prediction-2025/test.jsonl', lines=True)\n",
    "    \n",
    "    # Convertiamo subito il target in 1 (Vittoria) e 0 (Sconfitta)\n",
    "    if 'player_won' in train_df.columns:\n",
    "        train_df['player_won'] = train_df['player_won'].astype(int)\n",
    "    print(\"Caricamento riuscito!\")\n",
    "except Exception as e:\n",
    "    print(f\"Errore nel caricamento dati: {e}\")\n",
    "\n",
    "#####\n",
    "riga_da_rimuovere = 4877\n",
    "\n",
    "# Usiamo un controllo per sicurezza, nel caso la riga non esista\n",
    "if riga_da_rimuovere in train_df.index:\n",
    "    train_df = train_df.drop(riga_da_rimuovere)\n",
    "    print(f\"Riga {riga_da_rimuovere} rimossa con successo.\")\n",
    "else:\n",
    "    print(f\"Riga {riga_da_rimuovere} non trovata (forse giÃ  rimossa o non presente).\")\n",
    "\n",
    "filtro_livello_100 = train_df['p1_team_details'].apply(\n",
    "    lambda team_list: all(pokemon.get('level') == 100 for pokemon in team_list)\n",
    ")\n",
    "\n",
    "train_df = train_df[filtro_livello_100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T11:24:51.335988Z",
     "iopub.status.busy": "2025-11-13T11:24:51.335604Z",
     "iopub.status.idle": "2025-11-13T11:24:51.347297Z",
     "shell.execute_reply": "2025-11-13T11:24:51.345957Z",
     "shell.execute_reply.started": "2025-11-13T11:24:51.335958Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Mappa di efficacia dei tipi per la Generazione 1\n",
    "# Nota: 'Special' in Gen 1 copre sia Atk Sp. che Def Sp.\n",
    "# Non ci sono tipi Dark, Steel, o Fairy.\n",
    "TYPE_CHART_GEN1 = {\n",
    "    'NORMAL': {'ROCK': 0.5, 'GHOST': 0.0},\n",
    "    'FIRE': {'FIRE': 0.5, 'WATER': 0.5, 'GRASS': 2.0, 'ICE': 2.0, 'BUG': 2.0, 'ROCK': 0.5},\n",
    "    'WATER': {'FIRE': 2.0, 'WATER': 0.5, 'GRASS': 0.5, 'GROUND': 2.0, 'ROCK': 2.0, 'DRAGON': 0.5},\n",
    "    'ELECTRIC': {'WATER': 2.0, 'ELECTRIC': 0.5, 'GRASS': 0.5, 'GROUND': 0.0, 'FLYING': 2.0, 'DRAGON': 0.5},\n",
    "    'GRASS': {'FIRE': 0.5, 'WATER': 2.0, 'ELECTRIC': 1.0, 'GRASS': 0.5, 'POISON': 0.5, 'GROUND': 2.0, 'FLYING': 0.5, 'BUG': 0.5, 'ROCK': 2.0, 'DRAGON': 0.5},\n",
    "    'ICE': {'WATER': 0.5, 'GRASS': 2.0, 'ICE': 0.5, 'GROUND': 2.0, 'FLYING': 2.0, 'DRAGON': 2.0},\n",
    "    'FIGHTING': {'NORMAL': 2.0, 'POISON': 0.5, 'FLYING': 0.5, 'PSYCHIC': 0.5, 'BUG': 0.5, 'ROCK': 2.0, 'GHOST': 0.0},\n",
    "    'POISON': {'GRASS': 2.0, 'POISON': 0.5, 'GROUND': 0.5, 'BUG': 2.0, 'ROCK': 0.5, 'GHOST': 0.5},\n",
    "    'GROUND': {'FIRE': 2.0, 'ELECTRIC': 2.0, 'GRASS': 0.5, 'POISON': 2.0, 'FLYING': 0.0, 'BUG': 0.5, 'ROCK': 2.0},\n",
    "    'FLYING': {'ELECTRIC': 0.5, 'GRASS': 2.0, 'FIGHTING': 2.0, 'BUG': 2.0, 'ROCK': 0.5},\n",
    "    'PSYCHIC': {'FIGHTING': 2.0, 'POISON': 2.0, 'PSYCHIC': 0.5, 'GHOST': 1.0}, # In Gen 1, Psychic era immune a Ghost per un bug, ma i dati Showdown potrebbero averlo corretto. Assumiamo 1.0 per sicurezza, o 0.0 se il bug Ã¨ emulato. Qui usiamo 1.0.\n",
    "    'BUG': {'FIRE': 0.5, 'GRASS': 2.0, 'FIGHTING': 0.5, 'POISON': 2.0, 'FLYING': 0.5, 'PSYCHIC': 2.0},\n",
    "    'ROCK': {'FIRE': 2.0, 'ICE': 2.0, 'FIGHTING': 0.5, 'GROUND': 0.5, 'FLYING': 2.0, 'BUG': 2.0},\n",
    "    'GHOST': {'NORMAL': 0.0, 'PSYCHIC': 0.0, 'GHOST': 2.0}, # Famoso bug: Lick (Ghost) non colpisce Psychic.\n",
    "    'DRAGON': {'DRAGON': 2.0},\n",
    "}\n",
    "\n",
    "# Funzione helper per calcolare l'efficacia\n",
    "def get_type_effectiveness(move_type, target_types):\n",
    "    if move_type not in TYPE_CHART_GEN1:\n",
    "        return 1.0\n",
    "    \n",
    "    multiplier = 1.0\n",
    "    chart_for_move = TYPE_CHART_GEN1[move_type]\n",
    "    \n",
    "    for target_type in target_types:\n",
    "        if target_type in chart_for_move:\n",
    "            multiplier *= chart_for_move[target_type]\n",
    "            \n",
    "    return multiplier\n",
    "\n",
    "# PokÃ©mon dominanti nel metagame Gen 1 OU (S-Tier e A-Tier)\n",
    "# La loro presenza Ã¨ un segnale fortissimo.\n",
    "META_THREATS_GEN1 = {\n",
    "    'Snorlax', 'Tauros', 'Chansey', 'Alakazam', 'Starmie', 'Exeggutor', \n",
    "    'Zapdos', 'Jolteon', 'Rhydon', 'Golem', 'Lapras'\n",
    "}\n",
    "\n",
    "# Mosse di setup o status chiave\n",
    "STATUS_MOVES = {'Thunder Wave', 'Sleep Powder', 'Sing', 'Toxic', 'Lovely Kiss', 'Spore', 'Stun Spore', 'Glare'}\n",
    "SETUP_MOVES = {'Amnesia', 'Swords Dance', 'Agility', 'Growth'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T11:24:51.379606Z",
     "iopub.status.busy": "2025-11-13T11:24:51.379240Z",
     "iopub.status.idle": "2025-11-13T11:24:51.412864Z",
     "shell.execute_reply": "2025-11-13T11:24:51.411420Z",
     "shell.execute_reply.started": "2025-11-13T11:24:51.379585Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def create_advanced_features_1(df):\n",
    "    processed_data = []\n",
    "    for _, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Creazione features\"):\n",
    "        p1_team = row['p1_team_details']\n",
    "        p2_lead = row['p2_lead_details']\n",
    "        timeline = row['battle_timeline']\n",
    "        p1_lead = p1_team[0]\n",
    "        \n",
    "        feat_lead_speed_diff = p1_lead['base_spe'] - p2_lead['base_spe']\n",
    "        \n",
    "        p1_seen_status = {p['name']: {'hp_pct': 100, 'status': None} for p in p1_team}\n",
    "        p2_seen_status = {p2_lead['name']: {'hp_pct': 100, 'status': None}}\n",
    "        \n",
    "        feat_end_boost_diff = 0\n",
    "        feat_num_turns = 0\n",
    "        \n",
    "        if timeline:\n",
    "            feat_num_turns = timeline[-1].get('turn', 0)\n",
    "            for turn in timeline:\n",
    "                p1_state = turn.get('p1_pokemon_state')\n",
    "                if p1_state and p1_state.get('name'):\n",
    "                    p1_name = p1_state['name']\n",
    "                    p1_seen_status.setdefault(p1_name, {'hp_pct': 100, 'status': None})\n",
    "                    p1_seen_status[p1_name]['hp_pct'] = p1_state.get('hp_pct', p1_seen_status[p1_name]['hp_pct'])\n",
    "                    p1_seen_status[p1_name]['status'] = p1_state.get('status', p1_seen_status[p1_name]['status'])\n",
    "                    \n",
    "                p2_state = turn.get('p2_pokemon_state')\n",
    "                if p2_state and p2_state.get('name'):\n",
    "                    p2_name = p2_state['name']\n",
    "                    p2_seen_status.setdefault(p2_name, {'hp_pct': 100, 'status': None})\n",
    "                    p2_seen_status[p2_name]['hp_pct'] = p2_state.get('hp_pct', p2_seen_status[p2_name]['hp_pct'])\n",
    "                    p2_seen_status[p2_name]['status'] = p2_state.get('status', p2_seen_status[p2_name]['status'])\n",
    "\n",
    "                if turn.get('turn') == feat_num_turns:\n",
    "                    p1_boosts = sum(p1_state.get('boosts', {}).values()) if p1_state else 0\n",
    "                    p2_boosts = sum(p2_state.get('boosts', {}).values()) if p2_state else 0\n",
    "                    feat_end_boost_diff = p1_boosts - p2_boosts\n",
    "\n",
    "        p1_total_hp_seen = sum(p['hp_pct'] for p in p1_seen_status.values())\n",
    "        p2_total_hp_seen = sum(p['hp_pct'] for p in p2_seen_status.values())\n",
    "        feat_hp_advantage_seen = p1_total_hp_seen - p2_total_hp_seen\n",
    "        \n",
    "        feat_mons_revealed_diff = len(p2_seen_status) - len(p1_seen_status)\n",
    "        \n",
    "        p1_team_status_count = sum(1 for p in p1_seen_status.values() if p['status'] is not None)\n",
    "        p2_team_status_count = sum(1 for p in p2_seen_status.values() if p['status'] is not None)\n",
    "        feat_team_status_diff = p1_team_status_count - p2_team_status_count # (P1 status) - (P2 status)\n",
    "\n",
    "        processed_data.append({\n",
    "            'battle_id': row['battle_id'],\n",
    "            'p1_lead_name': p1_lead['name'], 'p2_lead_name': p2_lead['name'],\n",
    "            'lead_speed_diff': feat_lead_speed_diff,\n",
    "            'hp_advantage_seen': feat_hp_advantage_seen,\n",
    "            'mons_revealed_diff': feat_mons_revealed_diff,\n",
    "            'team_status_diff': feat_team_status_diff,\n",
    "            'end_boost_diff': feat_end_boost_diff,\n",
    "            'num_turns': feat_num_turns\n",
    "        })\n",
    "    return pd.DataFrame(processed_data).set_index('battle_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T11:24:51.414562Z",
     "iopub.status.busy": "2025-11-13T11:24:51.414259Z",
     "iopub.status.idle": "2025-11-13T11:24:53.466596Z",
     "shell.execute_reply": "2025-11-13T11:24:53.465178Z",
     "shell.execute_reply.started": "2025-11-13T11:24:51.414539Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"Inizio feature engineering avanzata sul set di training...\")\n",
    "X_train_features = create_advanced_features_1(train_df)\n",
    "\n",
    "print(\"\\nInizio feature engineering avanzata sul set di test...\")\n",
    "X_test_features = create_advanced_features_1(test_df)\n",
    "\n",
    "# Definiamo la nostra variabile target 'y'\n",
    "y_train = train_df.set_index('battle_id')['player_won']\n",
    "\n",
    "# Allineiamo X e y\n",
    "y_train = y_train.loc[X_train_features.index]\n",
    "\n",
    "print(\"\\nFeature engineering completato. Esempio di dati trasformati:\")\n",
    "print(X_train_features.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T11:24:53.467842Z",
     "iopub.status.busy": "2025-11-13T11:24:53.467576Z",
     "iopub.status.idle": "2025-11-13T11:24:53.486950Z",
     "shell.execute_reply": "2025-11-13T11:24:53.485474Z",
     "shell.execute_reply.started": "2025-11-13T11:24:53.467822Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "numeric_features = [\n",
    "    'lead_speed_diff',\n",
    "    'hp_advantage_seen',\n",
    "    'mons_revealed_diff',\n",
    "    'team_status_diff',\n",
    "    'end_boost_diff',\n",
    "    'num_turns'\n",
    "]\n",
    "categorical_features = ['p1_lead_name', 'p2_lead_name']\n",
    "\n",
    "# Creiamo i trasformatori (StandardScaler e OneHotEncoder)\n",
    "numeric_transformer = Pipeline(steps=[('scaler', RobustScaler())])\n",
    "categorical_transformer = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T11:33:52.563369Z",
     "iopub.status.busy": "2025-11-13T11:33:52.562950Z",
     "iopub.status.idle": "2025-11-13T11:33:52.671864Z",
     "shell.execute_reply": "2025-11-13T11:33:52.670171Z",
     "shell.execute_reply.started": "2025-11-13T11:33:52.563337Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Dividiamo i dati di training per una validazione locale\n",
    "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
    "    X_train_features, \n",
    "    y_train, \n",
    "    test_size=0.2, # 20% per la validazione\n",
    "    random_state=42,\n",
    "    stratify=y_train # Mantiene l'equilibrio delle classi\n",
    ")\n",
    "\n",
    "print(f\"Dimensione Training Split: {X_train_split.shape}\")\n",
    "print(f\"Dimensione Validation Split: {X_val_split.shape}\")\n",
    "\n",
    "# 1. Creiamo la pipeline con un modello \"di default\"\n",
    "# Usiamo C=1.0 come valore predefinito\n",
    "baseline_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(random_state=42, max_iter=1000, solver='liblinear', C=1.0))\n",
    "])\n",
    "\n",
    "# 2. Alleniamo il modello base SUL SOLO SET DI TRAINING SPLIT\n",
    "print(\"\\nAllenamento del modello baseline...\")\n",
    "baseline_pipeline.fit(X_train_split, y_train_split)\n",
    "\n",
    "# 3. Valutiamo il modello base SUL SET DI VALIDAZIONE\n",
    "y_val_pred = baseline_pipeline.predict(X_val_split)\n",
    "val_accuracy = accuracy_score(y_val_split, y_val_pred)\n",
    "\n",
    "print(f\"\\n--- Risultati Modello Baseline ---\")\n",
    "print(f\"Accuracy sul Validation Set: {val_accuracy:.4f}\")\n",
    "print(\"---------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T11:33:28.372061Z",
     "iopub.status.busy": "2025-11-13T11:33:28.371729Z",
     "iopub.status.idle": "2025-11-13T11:33:52.561024Z",
     "shell.execute_reply": "2025-11-13T11:33:52.560043Z",
     "shell.execute_reply.started": "2025-11-13T11:33:28.372037Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "print(\"\\nAvvio di GridSearchCV per l'ottimizzazione degli iperparametri...\")\n",
    "# 1. Creiamo la pipeline (la stessa di prima, ma senza 'C' definito)\n",
    "# La pipeline che verrÃ  testata da GridSearchCV\n",
    "tuning_pipeline_reg = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(random_state=42, max_iter=1000, solver='liblinear'))\n",
    "])\n",
    "\n",
    "# 2. Definiamo la griglia dei parametri\n",
    "# Vogliamo testare diversi valori per 'classifier__C'\n",
    "param_grid = {\n",
    "    'classifier__penalty': ['l1', 'l2'], \n",
    "    'classifier__C': [ 0.01, 0.1, 1, 10, 100],\n",
    "    'classifier__solver': ['liblinear'] \n",
    "}\n",
    "\n",
    "# 3. Impostiamo GridSearchCV\n",
    "# cv=5 significa 5-fold cross-validation\n",
    "# scoring='accuracy' Ã¨ la nostra metrica\n",
    "# n_jobs=-1 usa tutti i processori\n",
    "grid_search = GridSearchCV(\n",
    "    tuning_pipeline_reg, \n",
    "    param_grid, \n",
    "    cv=10, \n",
    "    scoring='accuracy', \n",
    "    n_jobs=-1,\n",
    "    verbose=1 # Mostra i log\n",
    ")\n",
    "\n",
    "# 4. Avviamo la ricerca sull'INTERO set di training\n",
    "# (GridSearchCV gestirÃ  internamente le divisioni di cross-validation)\n",
    "grid_search.fit(X_train_features, y_train)\n",
    "\n",
    "# 5. Analizziamo i risultati\n",
    "print(\"\\n--- Risultati GridSearchCV ---\")\n",
    "print(f\"Migliori parametri trovati: {grid_search.best_params_}\")\n",
    "print(f\"Migliore Accuracy (media CV): {grid_search.best_score_:.4f}\")\n",
    "print(\"------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T11:33:07.582034Z",
     "iopub.status.busy": "2025-11-13T11:33:07.581755Z",
     "iopub.status.idle": "2025-11-13T11:33:07.594248Z",
     "shell.execute_reply": "2025-11-13T11:33:07.593219Z",
     "shell.execute_reply.started": "2025-11-13T11:33:07.581985Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Estrai il modello migliore (la pipeline completa) da GridSearchCV\n",
    "final_model = grid_search.best_estimator_\n",
    "\n",
    "# 2. Estrai i nomi delle feature numeriche (li abbiamo giÃ )\n",
    "# Assicurati che 'numeric_features' sia la lista aggiornata che hai usato\n",
    "# numeric_features = ['lead_speed_diff', 'p1_lead_type_adv', ...]\n",
    "\n",
    "# 3. Estrai i nomi delle feature categoriche create dal OneHotEncoder\n",
    "# Questo Ã¨ il passaggio chiave\n",
    "try:\n",
    "    categorical_names = final_model.named_steps['preprocessor'] \\\n",
    "                                     .named_transformers_['cat'] \\\n",
    "                                     .named_steps['onehot'] \\\n",
    "                                     .get_feature_names_out(categorical_features)\n",
    "except AttributeError:\n",
    "    # Fallback per versioni piÃ¹ vecchie di scikit-learn\n",
    "    categorical_names = final_model.named_steps['preprocessor'] \\\n",
    "                                     .named_transformers_['cat'] \\\n",
    "                                     .named_steps['onehot'] \\\n",
    "                                     .get_feature_names_out()\n",
    "\n",
    "# 4. Combina tutti i nomi delle feature nell'ordine corretto\n",
    "all_feature_names = numeric_features + list(categorical_names)\n",
    "\n",
    "# 5. Estrai i coefficienti (l'importanza) dal modello di regressione logistica\n",
    "coefficients = final_model.named_steps['classifier'].coef_[0]\n",
    "\n",
    "# 6. Crea un DataFrame per visualizzarli in modo chiaro\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': all_feature_names,\n",
    "    'Coefficient': coefficients\n",
    "})\n",
    "\n",
    "# 7. Aggiungi il 'Coefficiente Assoluto' per ordinare per impatto (sia positivo che negativo)\n",
    "importance_df['Impact'] = importance_df['Coefficient'].abs()\n",
    "importance_df = importance_df.sort_values(by='Impact', ascending=False)\n",
    "\n",
    "# 8. Stampa i risultati\n",
    "print(\"--- Importanza delle Feature (Coefficienti del Modello) ---\")\n",
    "print(importance_df.to_string()) # .to_string() stampa tutto il DataFrame senza troncamenti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T11:26:07.231236Z",
     "iopub.status.busy": "2025-11-13T11:26:07.230984Z",
     "iopub.status.idle": "2025-11-13T11:26:07.296963Z",
     "shell.execute_reply": "2025-11-13T11:26:07.296224Z",
     "shell.execute_reply.started": "2025-11-13T11:26:07.231221Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 1. Estrai il modello finale\n",
    "final_model = grid_search.best_estimator_\n",
    "\n",
    "# 2. Genera le predizioni (saranno True/False)\n",
    "test_predictions_bool = final_model.predict(X_test_features)\n",
    "\n",
    "# 3. --- CORREZIONE 1: Converti True/False in 1/0 ---\n",
    "# .astype(int) converte True -> 1 e False -> 0\n",
    "test_predictions_int = test_predictions_bool.astype(int)\n",
    "\n",
    "# 4. Prendi i battle_id dall'indice\n",
    "test_battle_ids = X_test_features.index\n",
    "\n",
    "# 5. Crea il DataFrame con le due colonne CORRETTE\n",
    "submission_df = pd.DataFrame({\n",
    "    'battle_id': test_battle_ids,\n",
    "    'player_won': test_predictions_int  # Usa la versione 1/0\n",
    "})\n",
    "\n",
    "# 6. --- CORREZIONE 2: Salva SENZA l'indice di pandas ---\n",
    "# Aggiungendo 'index=False' si risolve il problema della \"stessa colonna\".\n",
    "submission_df.to_csv('submission_predictions.csv', index=False)\n",
    "\n",
    "print(\"\\n-------------------------------------------------\")\n",
    "print(\"File 'submission_predictions.csv' creato con successo!\")\n",
    "print(\"Ora conterrÃ  1 e 0, e colonne separate.\")\n",
    "print(\"-------------------------------------------------\")\n",
    "\n",
    "# Stampa un'anteprima\n",
    "print(submission_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T11:26:07.298164Z",
     "iopub.status.busy": "2025-11-13T11:26:07.297895Z",
     "iopub.status.idle": "2025-11-13T11:27:34.580989Z",
     "shell.execute_reply": "2025-11-13T11:27:34.579894Z",
     "shell.execute_reply.started": "2025-11-13T11:26:07.298143Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- 4. Funzione Feature Engineering (\"Full Story\") ---\n",
    "print(\"ðŸ§  Inizio feature engineering (Strategia 'Full Story')...\")\n",
    "\n",
    "def create_advanced_features_2(df):\n",
    "    \n",
    "    processed_data = []\n",
    "    \n",
    "    useless_leads = {\n",
    "        'Articuno', 'Golem', 'Rhydon', 'Lapras', 'Cloyster', \n",
    "        'Charizard', 'Victreebel', 'Dragonite', 'Gengar', 'Persian' \n",
    "    }\n",
    "    \n",
    "    for _, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Analisi 'Full Story'\"):\n",
    "        \n",
    "        p1_team = row['p1_team_details']\n",
    "        p2_lead = row['p2_lead_details']\n",
    "        timeline = row['battle_timeline']\n",
    "        p1_lead = p1_team[0]\n",
    "        \n",
    "        p1_lead_name = p1_lead['name'] if p1_lead['name'] not in useless_leads else 'Other'\n",
    "        p2_lead_name = p2_lead['name'] if p2_lead['name'] not in useless_leads else 'Other'\n",
    "            \n",
    "        # --- Feature Statiche (Turno 0) ---\n",
    "        feat_lead_speed_diff = p1_lead['base_spe'] - p2_lead['base_spe']\n",
    "        feat_p1_team_avg_speed = np.mean([p.get('base_spe', 70) for p in p1_team])\n",
    "        feat_p1_team_avg_bulk = np.mean([(p.get('base_hp', 80) + p.get('base_def', 80) + p.get('base_spa', 80)) for p in p1_team])\n",
    "        feat_p1_meta_threat_count = sum(1 for p in p1_team if p.get('name') in META_THREATS_GEN1)\n",
    "        feat_p2_lead_is_meta_threat = 1 if p2_lead['name'] in META_THREATS_GEN1 else 0\n",
    "        \n",
    "        # --- Feature Dinamiche (Turno 1-30) ---\n",
    "        p1_seen_status = {p['name']: {'hp_pct': 100, 'status': None} for p in p1_team}\n",
    "        p2_seen_status = {p2_lead['name']: {'hp_pct': 100, 'status': None}}\n",
    "        feat_end_boost_diff = 0\n",
    "        feat_p1_lead_stay_duration = 0\n",
    "        feat_p2_lead_forced_out = 0\n",
    "        feat_first_ko_turn = 0\n",
    "        feat_p1_setup_moves = 0\n",
    "        feat_p2_setup_moves = 0\n",
    "        \n",
    "        if timeline:\n",
    "            feat_num_turns = timeline[-1].get('turn', 0)\n",
    "            first_ko_achieved = False\n",
    "            last_p2_mon_name = p2_lead['name']\n",
    "            \n",
    "            for i, turn in enumerate(timeline):\n",
    "                p1_state = turn.get('p1_pokemon_state', {})\n",
    "                p2_state = turn.get('p2_pokemon_state', {})\n",
    "                p1_move = turn.get('p1_move_details')\n",
    "                p2_move = turn.get('p2_move_details')\n",
    "                \n",
    "                # Tracciamento HP/Status (con bug corretto)\n",
    "                if p1_state and p1_state.get('name'):\n",
    "                    p1_name = p1_state['name']\n",
    "                    p1_seen_status.setdefault(p1_name, {'hp_pct': 100, 'status': None})\n",
    "                    if p1_state.get('hp_pct') is not None:\n",
    "                        p1_seen_status[p1_name]['hp_pct'] = p1_state.get('hp_pct')\n",
    "                    p1_seen_status[p1_name]['status'] = p1_state.get('status')\n",
    "                if p2_state and p2_state.get('name'):\n",
    "                    p2_name = p2_state['name']\n",
    "                    p2_seen_status.setdefault(p2_name, {'hp_pct': 100, 'status': None})\n",
    "                    if p2_state.get('hp_pct') is not None:\n",
    "                        p2_seen_status[p2_name]['hp_pct'] = p2_state.get('hp_pct')\n",
    "                    p2_seen_status[p2_name]['status'] = p2_state.get('status')\n",
    "                \n",
    "                # Boost (solo ultimo turno)\n",
    "                if turn.get('turn') == feat_num_turns:\n",
    "                    feat_end_boost_diff = sum(p1_state.get('boosts', {}).values()) - sum(p2_state.get('boosts', {}).values())\n",
    "                \n",
    "                # Tracciamento Durata Lead P1\n",
    "                if p1_state.get('name') == p1_lead['name']:\n",
    "                    feat_p1_lead_stay_duration += 1\n",
    "                \n",
    "                # Tracciamento P2 Lead Forzato a Uscire\n",
    "                if i > 0 and p2_state.get('name') != last_p2_mon_name and last_p2_mon_name == p2_lead['name']:\n",
    "                    feat_p2_lead_forced_out = 1\n",
    "                if p2_state.get('name') != last_p2_mon_name:\n",
    "                    last_p2_mon_name = p2_state.get('name')\n",
    "                \n",
    "                # Tracciamento Primo KO\n",
    "                if not first_ko_achieved and (p1_state.get('hp_pct') == 0 or p2_state.get('hp_pct') == 0):\n",
    "                    feat_first_ko_turn = turn.get('turn', 0)\n",
    "                    first_ko_achieved = True\n",
    "                \n",
    "                # Tracciamento Setup Moves\n",
    "                if p1_move and p1_move.get('name') in SETUP_MOVES:\n",
    "                    feat_p1_setup_moves += 1\n",
    "                if p2_move and p2_move.get('name') in SETUP_MOVES:\n",
    "                    feat_p2_setup_moves += 1\n",
    "\n",
    "        feat_setup_advantage = feat_p1_setup_moves - feat_p2_setup_moves\n",
    "\n",
    "        # Feature \"Risultato Turno 30\" (le tue migliori)\n",
    "        p1_total_hp_seen = sum(p['hp_pct'] for p in p1_seen_status.values())\n",
    "        p2_total_hp_seen = sum(p['hp_pct'] for p in p2_seen_status.values())\n",
    "        feat_hp_advantage_seen = p1_total_hp_seen - p2_total_hp_seen\n",
    "        feat_mons_revealed_diff = len(p2_seen_status) - len(p1_seen_status)\n",
    "        \n",
    "        def calculate_status_score(status_dict):\n",
    "            score = 0\n",
    "            for p in status_dict.values():\n",
    "                status = p.get('status')\n",
    "                if status == 'slp': score += 3\n",
    "                elif status == 'par': score += 2\n",
    "                elif status in ['psn', 'tox']: score += 1\n",
    "            return score\n",
    "        p1_team_status_score = calculate_status_score(p1_seen_status)\n",
    "        p2_team_status_score = calculate_status_score(p2_seen_status)\n",
    "        feat_weighted_status_diff = p1_team_status_score - p2_team_status_score\n",
    "\n",
    "        processed_data.append({\n",
    "            'battle_id': row['battle_id'],\n",
    "            'p1_lead_name': p1_lead_name, \n",
    "            'p2_lead_name': p2_lead_name,\n",
    "            \n",
    "            # Statiche (Turno 0)\n",
    "            'lead_speed_diff': feat_lead_speed_diff,\n",
    "            'p1_team_avg_speed': feat_p1_team_avg_speed,\n",
    "            'p1_team_avg_bulk': feat_p1_team_avg_bulk,\n",
    "            'p1_meta_threat_count': feat_p1_meta_threat_count,\n",
    "            'p2_lead_is_meta_threat': feat_p2_lead_is_meta_threat,\n",
    "            \n",
    "            # Dinamiche \"Tempo\" (Turno 1-30)\n",
    "            'p1_lead_stay_duration': feat_p1_lead_stay_duration,\n",
    "            'p2_lead_forced_out': feat_p2_lead_forced_out,\n",
    "            'first_ko_turn': feat_first_ko_turn,\n",
    "            'setup_advantage': feat_setup_advantage,\n",
    "            \n",
    "            # Dinamiche \"Risultato\" (Turno 30)\n",
    "            'hp_advantage_seen': feat_hp_advantage_seen,\n",
    "            'mons_revealed_diff': feat_mons_revealed_diff,\n",
    "            'weighted_status_diff': feat_weighted_status_diff,\n",
    "            'end_boost_diff': feat_end_boost_diff,\n",
    "        })\n",
    "    return pd.DataFrame(processed_data).set_index('battle_id')\n",
    "\n",
    "# Esegui la creazione delle feature\n",
    "X_train_features = create_advanced_features_2(train_df)\n",
    "X_test_features = create_advanced_features_2(test_df)\n",
    "y_train = train_df.set_index('battle_id')['player_won'].loc[X_train_features.index]\n",
    "print(\"Feature engineering ('Full Story') completato.\")\n",
    "\n",
    "# --- 5. Definizione Liste Feature (TUTTE) ---\n",
    "numeric_features = [\n",
    "    # Statiche\n",
    "    'lead_speed_diff',\n",
    "    'p1_team_avg_speed',\n",
    "    'p1_team_avg_bulk',\n",
    "    'p1_meta_threat_count',\n",
    "    'p2_lead_is_meta_threat',\n",
    "    \n",
    "    # Dinamiche \"Tempo\"\n",
    "    'p1_lead_stay_duration',\n",
    "    'p2_lead_forced_out',\n",
    "    'first_ko_turn',\n",
    "    'setup_advantage',\n",
    "    \n",
    "    # Dinamiche \"Risultato\"\n",
    "    'hp_advantage_seen',\n",
    "    'mons_revealed_diff',\n",
    "    'weighted_status_diff',\n",
    "    'end_boost_diff',\n",
    "]\n",
    "categorical_features = ['p1_lead_name', 'p2_lead_name']\n",
    "\n",
    "# --- 6. Creazione Pipeline (per XGBoost) ---\n",
    "print(\"ðŸ› ï¸  Costruzione pipeline XGBoost...\")\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[('scaler', 'passthrough')]) \n",
    "categorical_transformer = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "tuning_pipeline_xgb = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', XGBClassifier(random_state=42, eval_metric='logloss')) \n",
    "])\n",
    "\n",
    "# --- 7. PARAM_GRID (Ottimizzazione Avanzata) --- {'classifier__colsample_bytree': 0.8, 'classifier__gamma': 0.1,\n",
    "# 'classifier__learning_rate': 0.05, 'classifier__max_depth': 4, 'classifier__n_estimators': 500, 'classifier__subsample': 0.7}\n",
    "#{'classifier__colsample_bytree': 0.9, 'classifier__gamma': 0.2, 'classifier__learning_rate': 0.03, 'classifier__max_depth': 5,\n",
    " #'classifier__n_estimators': 600, 'classifier__subsample': 0.8}\n",
    "print(\"âš™ï¸ Definizione della griglia di iperparametri avanzata...\")\n",
    "param_grid = {\n",
    "    'classifier__n_estimators': [500, 700],\n",
    "    'classifier__learning_rate': [0.02, 0.08],\n",
    "    'classifier__max_depth': [3, 5], # Manteniamo la profonditÃ  bassa per evitare overfitting\n",
    "    'classifier__subsample': [0.6, 0.9],\n",
    "    'classifier__colsample_bytree': [0.75, 1],\n",
    "    'classifier__gamma': [0.1, 0.3] # Regolarizzazione piÃ¹ forte\n",
    "}\n",
    "\n",
    "# --- 8. Addestramento GridSearchCV ---\n",
    "print(\"ðŸš€ Inizio addestramento (GridSearchCV 'Full Story')...\")\n",
    "print(\"âš ï¸ Questo richiederÃ  molto tempo!\")\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    tuning_pipeline_xgb, \n",
    "    param_grid, \n",
    "    cv=5, \n",
    "    scoring='accuracy', \n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "grid_search.fit(X_train_features, y_train)\n",
    "\n",
    "print(\"\\n--- âœ… Risultati Finali ---\")\n",
    "print(f\"Migliori parametri trovati: {grid_search.best_params_}\")\n",
    "print(f\"Accuracy FINALE (media CV): {grid_search.best_score_ * 100:.2f}%\")\n",
    "# --- 9. Creazione File CSV Finale (submission_predictions.csv) ---\n",
    "print(\"\\nðŸ“„ Creazione file 'submission_predictions.csv'...\")\n",
    "\n",
    "final_model = grid_search.best_estimator_\n",
    "test_predictions = final_model.predict(X_test_features)\n",
    "test_battle_ids = X_test_features.index\n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    'battle_id': test_battle_ids,\n",
    "    'player_won': test_predictions.astype(int)\n",
    "})\n",
    "\n",
    "submission_df.to_csv('submission_predictions.csv', index=False)\n",
    "\n",
    "print(\"-------------------------------------------------\")\n",
    "print(\"File 'submission_predictions.csv' creato con successo!\")\n",
    "print(\"Lo troverai nel pannello 'Data' -> 'Output' sulla destra.\")\n",
    "print(\"-------------------------------------------------\")\n",
    "print(\"\\nAnteprima del file:\")\n",
    "print(submission_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T11:36:46.336141Z",
     "iopub.status.busy": "2025-11-13T11:36:46.335864Z",
     "iopub.status.idle": "2025-11-13T11:37:02.865561Z",
     "shell.execute_reply": "2025-11-13T11:37:02.864570Z",
     "shell.execute_reply.started": "2025-11-13T11:36:46.336127Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- 2. Creazione Pipeline (per Stacking) ---\n",
    "print(\"ðŸ› ï¸  Costruzione pipeline Stacking...\")\n",
    "\n",
    "# --- Pipeline 1: XGBoost (Modello non lineare) ---\n",
    "# Usa il pre-processore che non standardizza ('passthrough')\n",
    "preprocessor_xgb = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', 'passthrough', numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "# Usa i parametri migliori che hai trovato per 83.95% #{'classifier__colsample_bytree': 0.9, 'classifier__gamma': 0.2, 'classifier__learning_rate': 0.03, 'classifier__max_depth': 5,\n",
    " #'classifier__n_estimators': 600, 'classifier__subsample': 0.8}\n",
    "#{'classifier__colsample_bytree': 1, 'classifier__gamma': 0.3, 'classifier__learning_rate': 0.02, 'classifier__max_depth': 5, 'classifier__n_estimators': 700, 'classifier__subsample': 0.9}\n",
    "\n",
    "clf_xgb = XGBClassifier(random_state=42, \n",
    "                        eval_metric='logloss',\n",
    "                        n_estimators=600,\n",
    "                        learning_rate=0.03,\n",
    "                        max_depth=5,\n",
    "                        subsample=0.8,\n",
    "                        colsample_bytree=0.9,\n",
    "                        gamma=0.2)\n",
    "pipeline_xgb = Pipeline(steps=[('preprocessor', preprocessor_xgb),\n",
    "                               ('classifier', clf_xgb)])\n",
    "\n",
    "# --- Pipeline 2: Logistic Regression (Modello lineare) ---\n",
    "# DEVE usare StandardScaler\n",
    "numeric_transformer_logreg = Pipeline(steps=[('scaler', StandardScaler())])\n",
    "categorical_transformer_logreg = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))])\n",
    "preprocessor_logreg = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer_logreg, numeric_features),\n",
    "        ('cat', categorical_transformer_logreg, categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "# Usa i parametri migliori che hai trovato per il 73%{'classifier__C': 1, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear'}\n",
    "clf_logreg = LogisticRegression(C=10, solver='liblinear', random_state=42, penalty='l1') \n",
    "pipeline_logreg = Pipeline(steps=[('preprocessor', preprocessor_logreg),\n",
    "                                  ('classifier', clf_logreg)])\n",
    "\n",
    "# --- Livello Finale: Stacking (Il \"Meta-Modello\") ---\n",
    "print(\"ðŸ§  Creazione del Meta-Modello...\")\n",
    "# Creiamo il \"Meta-Modello\" (Il Capo)\n",
    "meta_model = LogisticRegression(C=1.0, random_state=42)\n",
    "\n",
    "# Creiamo il modello Stacked\n",
    "stacking_clf = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('xgb', pipeline_xgb), \n",
    "        ('logreg', pipeline_logreg) # Combina i due modelli DIVERSI\n",
    "    ], \n",
    "    final_estimator=meta_model, # Questo Ã¨ il tuo \"Meta-Modello\"\n",
    "    passthrough=False, # Diamo al \"Capo\" solo le previsioni (corregge l'errore 'starmie')\n",
    "    cv=5,              # Cruciale per un addestramento robusto\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# --- 3. Calcolo dell'Accuracy (Cross-Validation) ---\n",
    "print(\"ðŸš€ Avvio della Cross-Validation per il Meta-Modello...\")\n",
    "print(\"Questo richiederÃ  MOLTO tempo...\")\n",
    "\n",
    "scores = cross_val_score(\n",
    "    stacking_clf,                \n",
    "    X_train_features,     \n",
    "    y_train,              \n",
    "    cv=5,                 \n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"\\n--- âœ… Risultati della Cross-Validation ---\")\n",
    "print(f\"Accuratezza per ognuno dei 5 test: {scores}\")\n",
    "print(f\"Accuracy MEDIA FINALE: {scores.mean() * 100:.2f}%\")\n",
    "print(f\"Deviazione Standard: {scores.std() * 100:.2f}%\")\n",
    "\n",
    "\n",
    "# --- 4. Addestramento Finale (per la Submission) ---\n",
    "print(\"\\nðŸš€ Inizio addestramento finale (Meta-Modello) sull'intero set di training...\")\n",
    "stacking_clf.fit(X_train_features, y_train)\n",
    "print(\"Addestramento completato.\")\n",
    "\n",
    "# --- 5. Creazione File CSV Finale (submission_predictions.csv) ---\n",
    "print(\"\\nðŸ“„ Creazione file 'submission_predictions.csv'...\")\n",
    "\n",
    "final_model = stacking_clf # Il Meta-Modello Ã¨ il modello finale\n",
    "test_predictions = final_model.predict(X_test_features)\n",
    "test_battle_ids = X_test_features.index\n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    'battle_id': test_battle_ids,\n",
    "    'player_won': test_predictions.astype(int)\n",
    "})\n",
    "\n",
    "submission_df.to_csv('submission_predictions.csv', index=False)\n",
    "\n",
    "print(\"-------------------------------------------------\")\n",
    "print(\"File 'submission_predictions.csv' creato con successo!\")\n",
    "print(\"Lo troverai nel pannello 'Data' -> 'Output' sulla destra.\")\n",
    "print(\"-------------------------------------------------\")\n",
    "print(\"\\nAnteprima del file:\")\n",
    "print(submission_df.head())"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 13033998,
     "sourceId": 107555,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
