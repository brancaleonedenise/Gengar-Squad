{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":107555,"databundleVersionId":13033998,"sourceType":"competition"}],"dockerImageVersionId":31192,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"****Caricamento test.jsonl e train.jsonl****","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport json\nimport warnings\nfrom tqdm.auto import tqdm\n\n# --- 1. Import di Scikit-learn ---\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression   # <-- Il tuo modello\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, RobustScaler # <-- StandardScaler Ã¨ incluso\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.exceptions import ConvergenceWarning\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.model_selection import cross_val_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T11:24:40.450116Z","iopub.execute_input":"2025-11-13T11:24:40.450455Z","iopub.status.idle":"2025-11-13T11:24:40.458280Z","shell.execute_reply.started":"2025-11-13T11:24:40.450432Z","shell.execute_reply":"2025-11-13T11:24:40.456728Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"try:\n    train_df = pd.read_json('/kaggle/input/fds-pokemon-battles-prediction-2025/train.jsonl', lines=True)\n    test_df = pd.read_json('/kaggle/input/fds-pokemon-battles-prediction-2025/test.jsonl', lines=True)\n    \n    # Convertiamo subito il target in 1 (Vittoria) e 0 (Sconfitta)\n    if 'player_won' in train_df.columns:\n        train_df['player_won'] = train_df['player_won'].astype(int)\n    print(\"Caricamento riuscito!\")\nexcept Exception as e:\n    print(f\"Errore nel caricamento dati: {e}\")\n\n#####\nriga_da_rimuovere = 4877\n\n# Usiamo un controllo per sicurezza, nel caso la riga non esista\nif riga_da_rimuovere in train_df.index:\n    train_df = train_df.drop(riga_da_rimuovere)\n    print(f\"Riga {riga_da_rimuovere} rimossa con successo.\")\nelse:\n    print(f\"Riga {riga_da_rimuovere} non trovata (forse giÃ  rimossa o non presente).\")\n\nfiltro_livello_100 = train_df['p1_team_details'].apply(\n    lambda team_list: all(pokemon.get('level') == 100 for pokemon in team_list)\n)\n\ntrain_df = train_df[filtro_livello_100]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T11:24:40.460535Z","iopub.execute_input":"2025-11-13T11:24:40.461657Z","iopub.status.idle":"2025-11-13T11:24:51.333892Z","shell.execute_reply.started":"2025-11-13T11:24:40.461615Z","shell.execute_reply":"2025-11-13T11:24:51.332481Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Mappa di efficacia dei tipi per la Generazione 1\n# Nota: 'Special' in Gen 1 copre sia Atk Sp. che Def Sp.\n# Non ci sono tipi Dark, Steel, o Fairy.\nTYPE_CHART_GEN1 = {\n    'NORMAL': {'ROCK': 0.5, 'GHOST': 0.0},\n    'FIRE': {'FIRE': 0.5, 'WATER': 0.5, 'GRASS': 2.0, 'ICE': 2.0, 'BUG': 2.0, 'ROCK': 0.5},\n    'WATER': {'FIRE': 2.0, 'WATER': 0.5, 'GRASS': 0.5, 'GROUND': 2.0, 'ROCK': 2.0, 'DRAGON': 0.5},\n    'ELECTRIC': {'WATER': 2.0, 'ELECTRIC': 0.5, 'GRASS': 0.5, 'GROUND': 0.0, 'FLYING': 2.0, 'DRAGON': 0.5},\n    'GRASS': {'FIRE': 0.5, 'WATER': 2.0, 'ELECTRIC': 1.0, 'GRASS': 0.5, 'POISON': 0.5, 'GROUND': 2.0, 'FLYING': 0.5, 'BUG': 0.5, 'ROCK': 2.0, 'DRAGON': 0.5},\n    'ICE': {'WATER': 0.5, 'GRASS': 2.0, 'ICE': 0.5, 'GROUND': 2.0, 'FLYING': 2.0, 'DRAGON': 2.0},\n    'FIGHTING': {'NORMAL': 2.0, 'POISON': 0.5, 'FLYING': 0.5, 'PSYCHIC': 0.5, 'BUG': 0.5, 'ROCK': 2.0, 'GHOST': 0.0},\n    'POISON': {'GRASS': 2.0, 'POISON': 0.5, 'GROUND': 0.5, 'BUG': 2.0, 'ROCK': 0.5, 'GHOST': 0.5},\n    'GROUND': {'FIRE': 2.0, 'ELECTRIC': 2.0, 'GRASS': 0.5, 'POISON': 2.0, 'FLYING': 0.0, 'BUG': 0.5, 'ROCK': 2.0},\n    'FLYING': {'ELECTRIC': 0.5, 'GRASS': 2.0, 'FIGHTING': 2.0, 'BUG': 2.0, 'ROCK': 0.5},\n    'PSYCHIC': {'FIGHTING': 2.0, 'POISON': 2.0, 'PSYCHIC': 0.5, 'GHOST': 1.0}, # In Gen 1, Psychic era immune a Ghost per un bug, ma i dati Showdown potrebbero averlo corretto. Assumiamo 1.0 per sicurezza, o 0.0 se il bug Ã¨ emulato. Qui usiamo 1.0.\n    'BUG': {'FIRE': 0.5, 'GRASS': 2.0, 'FIGHTING': 0.5, 'POISON': 2.0, 'FLYING': 0.5, 'PSYCHIC': 2.0},\n    'ROCK': {'FIRE': 2.0, 'ICE': 2.0, 'FIGHTING': 0.5, 'GROUND': 0.5, 'FLYING': 2.0, 'BUG': 2.0},\n    'GHOST': {'NORMAL': 0.0, 'PSYCHIC': 0.0, 'GHOST': 2.0}, # Famoso bug: Lick (Ghost) non colpisce Psychic.\n    'DRAGON': {'DRAGON': 2.0},\n}\n\n# Funzione helper per calcolare l'efficacia\ndef get_type_effectiveness(move_type, target_types):\n    if move_type not in TYPE_CHART_GEN1:\n        return 1.0\n    \n    multiplier = 1.0\n    chart_for_move = TYPE_CHART_GEN1[move_type]\n    \n    for target_type in target_types:\n        if target_type in chart_for_move:\n            multiplier *= chart_for_move[target_type]\n            \n    return multiplier\n\n# PokÃ©mon dominanti nel metagame Gen 1 OU (S-Tier e A-Tier)\n# La loro presenza Ã¨ un segnale fortissimo.\nMETA_THREATS_GEN1 = {\n    'Snorlax', 'Tauros', 'Chansey', 'Alakazam', 'Starmie', 'Exeggutor', \n    'Zapdos', 'Jolteon', 'Rhydon', 'Golem', 'Lapras'\n}\n\n# Mosse di setup o status chiave\nSTATUS_MOVES = {'Thunder Wave', 'Sleep Powder', 'Sing', 'Toxic', 'Lovely Kiss', 'Spore', 'Stun Spore', 'Glare'}\nSETUP_MOVES = {'Amnesia', 'Swords Dance', 'Agility', 'Growth'}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T11:24:51.335604Z","iopub.execute_input":"2025-11-13T11:24:51.335988Z","iopub.status.idle":"2025-11-13T11:24:51.347297Z","shell.execute_reply.started":"2025-11-13T11:24:51.335958Z","shell.execute_reply":"2025-11-13T11:24:51.345957Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_advanced_features_1(df):\n    processed_data = []\n    for _, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Creazione features\"):\n        p1_team = row['p1_team_details']\n        p2_lead = row['p2_lead_details']\n        timeline = row['battle_timeline']\n        p1_lead = p1_team[0]\n        \n        feat_lead_speed_diff = p1_lead['base_spe'] - p2_lead['base_spe']\n        \n        p1_seen_status = {p['name']: {'hp_pct': 100, 'status': None} for p in p1_team}\n        p2_seen_status = {p2_lead['name']: {'hp_pct': 100, 'status': None}}\n        \n        feat_end_boost_diff = 0\n        feat_num_turns = 0\n        \n        if timeline:\n            feat_num_turns = timeline[-1].get('turn', 0)\n            for turn in timeline:\n                p1_state = turn.get('p1_pokemon_state')\n                if p1_state and p1_state.get('name'):\n                    p1_name = p1_state['name']\n                    p1_seen_status.setdefault(p1_name, {'hp_pct': 100, 'status': None})\n                    p1_seen_status[p1_name]['hp_pct'] = p1_state.get('hp_pct', p1_seen_status[p1_name]['hp_pct'])\n                    p1_seen_status[p1_name]['status'] = p1_state.get('status', p1_seen_status[p1_name]['status'])\n                    \n                p2_state = turn.get('p2_pokemon_state')\n                if p2_state and p2_state.get('name'):\n                    p2_name = p2_state['name']\n                    p2_seen_status.setdefault(p2_name, {'hp_pct': 100, 'status': None})\n                    p2_seen_status[p2_name]['hp_pct'] = p2_state.get('hp_pct', p2_seen_status[p2_name]['hp_pct'])\n                    p2_seen_status[p2_name]['status'] = p2_state.get('status', p2_seen_status[p2_name]['status'])\n\n                if turn.get('turn') == feat_num_turns:\n                    p1_boosts = sum(p1_state.get('boosts', {}).values()) if p1_state else 0\n                    p2_boosts = sum(p2_state.get('boosts', {}).values()) if p2_state else 0\n                    feat_end_boost_diff = p1_boosts - p2_boosts\n\n        p1_total_hp_seen = sum(p['hp_pct'] for p in p1_seen_status.values())\n        p2_total_hp_seen = sum(p['hp_pct'] for p in p2_seen_status.values())\n        feat_hp_advantage_seen = p1_total_hp_seen - p2_total_hp_seen\n        \n        feat_mons_revealed_diff = len(p2_seen_status) - len(p1_seen_status)\n        \n        p1_team_status_count = sum(1 for p in p1_seen_status.values() if p['status'] is not None)\n        p2_team_status_count = sum(1 for p in p2_seen_status.values() if p['status'] is not None)\n        feat_team_status_diff = p1_team_status_count - p2_team_status_count # (P1 status) - (P2 status)\n\n        processed_data.append({\n            'battle_id': row['battle_id'],\n            'p1_lead_name': p1_lead['name'], 'p2_lead_name': p2_lead['name'],\n            'lead_speed_diff': feat_lead_speed_diff,\n            'hp_advantage_seen': feat_hp_advantage_seen,\n            'mons_revealed_diff': feat_mons_revealed_diff,\n            'team_status_diff': feat_team_status_diff,\n            'end_boost_diff': feat_end_boost_diff,\n            'num_turns': feat_num_turns\n        })\n    return pd.DataFrame(processed_data).set_index('battle_id')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T11:24:51.379240Z","iopub.execute_input":"2025-11-13T11:24:51.379606Z","iopub.status.idle":"2025-11-13T11:24:51.412864Z","shell.execute_reply.started":"2025-11-13T11:24:51.379585Z","shell.execute_reply":"2025-11-13T11:24:51.411420Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Inizio feature engineering avanzata sul set di training...\")\nX_train_features = create_advanced_features_1(train_df)\n\nprint(\"\\nInizio feature engineering avanzata sul set di test...\")\nX_test_features = create_advanced_features_1(test_df)\n\n# Definiamo la nostra variabile target 'y'\ny_train = train_df.set_index('battle_id')['player_won']\n\n# Allineiamo X e y\ny_train = y_train.loc[X_train_features.index]\n\nprint(\"\\nFeature engineering completato. Esempio di dati trasformati:\")\nprint(X_train_features.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T11:24:51.414259Z","iopub.execute_input":"2025-11-13T11:24:51.414562Z","iopub.status.idle":"2025-11-13T11:24:53.466596Z","shell.execute_reply.started":"2025-11-13T11:24:51.414539Z","shell.execute_reply":"2025-11-13T11:24:53.465178Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"numeric_features = [\n    'lead_speed_diff',\n    'hp_advantage_seen',\n    'mons_revealed_diff',\n    'team_status_diff',\n    'end_boost_diff',\n    'num_turns'\n]\ncategorical_features = ['p1_lead_name', 'p2_lead_name']\n\n# Creiamo i trasformatori (StandardScaler e OneHotEncoder)\nnumeric_transformer = Pipeline(steps=[('scaler', RobustScaler())])\ncategorical_transformer = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features),\n        ('cat', categorical_transformer, categorical_features)\n    ],\n    remainder='passthrough'\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T11:24:53.467576Z","iopub.execute_input":"2025-11-13T11:24:53.467842Z","iopub.status.idle":"2025-11-13T11:24:53.486950Z","shell.execute_reply.started":"2025-11-13T11:24:53.467822Z","shell.execute_reply":"2025-11-13T11:24:53.485474Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Dividiamo i dati di training per una validazione locale\nX_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n    X_train_features, \n    y_train, \n    test_size=0.2, # 20% per la validazione\n    random_state=42,\n    stratify=y_train # Mantiene l'equilibrio delle classi\n)\n\nprint(f\"Dimensione Training Split: {X_train_split.shape}\")\nprint(f\"Dimensione Validation Split: {X_val_split.shape}\")\n\n# 1. Creiamo la pipeline con un modello \"di default\"\n# Usiamo C=1.0 come valore predefinito\nbaseline_pipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('classifier', LogisticRegression(random_state=42, max_iter=1000, solver='liblinear', C=1.0))\n])\n\n# 2. Alleniamo il modello base SUL SOLO SET DI TRAINING SPLIT\nprint(\"\\nAllenamento del modello baseline...\")\nbaseline_pipeline.fit(X_train_split, y_train_split)\n\n# 3. Valutiamo il modello base SUL SET DI VALIDAZIONE\ny_val_pred = baseline_pipeline.predict(X_val_split)\nval_accuracy = accuracy_score(y_val_split, y_val_pred)\n\nprint(f\"\\n--- Risultati Modello Baseline ---\")\nprint(f\"Accuracy sul Validation Set: {val_accuracy:.4f}\")\nprint(\"---------------------------------\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T11:33:52.562950Z","iopub.execute_input":"2025-11-13T11:33:52.563369Z","iopub.status.idle":"2025-11-13T11:33:52.671864Z","shell.execute_reply.started":"2025-11-13T11:33:52.563337Z","shell.execute_reply":"2025-11-13T11:33:52.670171Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nprint(\"\\nAvvio di GridSearchCV per l'ottimizzazione degli iperparametri...\")\n# 1. Creiamo la pipeline (la stessa di prima, ma senza 'C' definito)\n# La pipeline che verrÃ  testata da GridSearchCV\ntuning_pipeline_reg = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('classifier', LogisticRegression(random_state=42, max_iter=1000, solver='liblinear'))\n])\n\n# 2. Definiamo la griglia dei parametri\n# Vogliamo testare diversi valori per 'classifier__C'\nparam_grid = {\n    'classifier__penalty': ['l1', 'l2'], \n    'classifier__C': [ 0.01, 0.1, 1, 10, 100],\n    'classifier__solver': ['liblinear'] \n}\n\n# 3. Impostiamo GridSearchCV\n# cv=5 significa 5-fold cross-validation\n# scoring='accuracy' Ã¨ la nostra metrica\n# n_jobs=-1 usa tutti i processori\ngrid_search = GridSearchCV(\n    tuning_pipeline_reg, \n    param_grid, \n    cv=10, \n    scoring='accuracy', \n    n_jobs=-1,\n    verbose=1 # Mostra i log\n)\n\n# 4. Avviamo la ricerca sull'INTERO set di training\n# (GridSearchCV gestirÃ  internamente le divisioni di cross-validation)\ngrid_search.fit(X_train_features, y_train)\n\n# 5. Analizziamo i risultati\nprint(\"\\n--- Risultati GridSearchCV ---\")\nprint(f\"Migliori parametri trovati: {grid_search.best_params_}\")\nprint(f\"Migliore Accuracy (media CV): {grid_search.best_score_:.4f}\")\nprint(\"------------------------------\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T11:33:28.371729Z","iopub.execute_input":"2025-11-13T11:33:28.372061Z","iopub.status.idle":"2025-11-13T11:33:52.561024Z","shell.execute_reply.started":"2025-11-13T11:33:28.372037Z","shell.execute_reply":"2025-11-13T11:33:52.560043Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# 1. Estrai il modello migliore (la pipeline completa) da GridSearchCV\nfinal_model = grid_search.best_estimator_\n\n# 2. Estrai i nomi delle feature numeriche (li abbiamo giÃ )\n# Assicurati che 'numeric_features' sia la lista aggiornata che hai usato\n# numeric_features = ['lead_speed_diff', 'p1_lead_type_adv', ...]\n\n# 3. Estrai i nomi delle feature categoriche create dal OneHotEncoder\n# Questo Ã¨ il passaggio chiave\ntry:\n    categorical_names = final_model.named_steps['preprocessor'] \\\n                                     .named_transformers_['cat'] \\\n                                     .named_steps['onehot'] \\\n                                     .get_feature_names_out(categorical_features)\nexcept AttributeError:\n    # Fallback per versioni piÃ¹ vecchie di scikit-learn\n    categorical_names = final_model.named_steps['preprocessor'] \\\n                                     .named_transformers_['cat'] \\\n                                     .named_steps['onehot'] \\\n                                     .get_feature_names_out()\n\n# 4. Combina tutti i nomi delle feature nell'ordine corretto\nall_feature_names = numeric_features + list(categorical_names)\n\n# 5. Estrai i coefficienti (l'importanza) dal modello di regressione logistica\ncoefficients = final_model.named_steps['classifier'].coef_[0]\n\n# 6. Crea un DataFrame per visualizzarli in modo chiaro\nimportance_df = pd.DataFrame({\n    'Feature': all_feature_names,\n    'Coefficient': coefficients\n})\n\n# 7. Aggiungi il 'Coefficiente Assoluto' per ordinare per impatto (sia positivo che negativo)\nimportance_df['Impact'] = importance_df['Coefficient'].abs()\nimportance_df = importance_df.sort_values(by='Impact', ascending=False)\n\n# 8. Stampa i risultati\nprint(\"--- Importanza delle Feature (Coefficienti del Modello) ---\")\nprint(importance_df.to_string()) # .to_string() stampa tutto il DataFrame senza troncamenti","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T11:33:07.581755Z","iopub.execute_input":"2025-11-13T11:33:07.582034Z","iopub.status.idle":"2025-11-13T11:33:07.594248Z","shell.execute_reply.started":"2025-11-13T11:33:07.581985Z","shell.execute_reply":"2025-11-13T11:33:07.593219Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1. Estrai il modello finale\nfinal_model = grid_search.best_estimator_\n\n# 2. Genera le predizioni (saranno True/False)\ntest_predictions_bool = final_model.predict(X_test_features)\n\n# 3. --- CORREZIONE 1: Converti True/False in 1/0 ---\n# .astype(int) converte True -> 1 e False -> 0\ntest_predictions_int = test_predictions_bool.astype(int)\n\n# 4. Prendi i battle_id dall'indice\ntest_battle_ids = X_test_features.index\n\n# 5. Crea il DataFrame con le due colonne CORRETTE\nsubmission_df = pd.DataFrame({\n    'battle_id': test_battle_ids,\n    'player_won': test_predictions_int  # Usa la versione 1/0\n})\n\n# 6. --- CORREZIONE 2: Salva SENZA l'indice di pandas ---\n# Aggiungendo 'index=False' si risolve il problema della \"stessa colonna\".\nsubmission_df.to_csv('submission_predictions.csv', index=False)\n\nprint(\"\\n-------------------------------------------------\")\nprint(\"File 'submission_predictions.csv' creato con successo!\")\nprint(\"Ora conterrÃ  1 e 0, e colonne separate.\")\nprint(\"-------------------------------------------------\")\n\n# Stampa un'anteprima\nprint(submission_df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T11:26:07.230984Z","iopub.execute_input":"2025-11-13T11:26:07.231236Z","iopub.status.idle":"2025-11-13T11:26:07.296963Z","shell.execute_reply.started":"2025-11-13T11:26:07.231221Z","shell.execute_reply":"2025-11-13T11:26:07.296224Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 4. Funzione Feature Engineering (\"Full Story\") ---\nprint(\"ðŸ§  Inizio feature engineering (Strategia 'Full Story')...\")\n\ndef create_advanced_features_2(df):\n    \n    processed_data = []\n    \n    useless_leads = {\n        'Articuno', 'Golem', 'Rhydon', 'Lapras', 'Cloyster', \n        'Charizard', 'Victreebel', 'Dragonite', 'Gengar', 'Persian' \n    }\n    \n    for _, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Analisi 'Full Story'\"):\n        \n        p1_team = row['p1_team_details']\n        p2_lead = row['p2_lead_details']\n        timeline = row['battle_timeline']\n        p1_lead = p1_team[0]\n        \n        p1_lead_name = p1_lead['name'] if p1_lead['name'] not in useless_leads else 'Other'\n        p2_lead_name = p2_lead['name'] if p2_lead['name'] not in useless_leads else 'Other'\n            \n        # --- Feature Statiche (Turno 0) ---\n        feat_lead_speed_diff = p1_lead['base_spe'] - p2_lead['base_spe']\n        feat_p1_team_avg_speed = np.mean([p.get('base_spe', 70) for p in p1_team])\n        feat_p1_team_avg_bulk = np.mean([(p.get('base_hp', 80) + p.get('base_def', 80) + p.get('base_spa', 80)) for p in p1_team])\n        feat_p1_meta_threat_count = sum(1 for p in p1_team if p.get('name') in META_THREATS_GEN1)\n        feat_p2_lead_is_meta_threat = 1 if p2_lead['name'] in META_THREATS_GEN1 else 0\n        \n        # --- Feature Dinamiche (Turno 1-30) ---\n        p1_seen_status = {p['name']: {'hp_pct': 100, 'status': None} for p in p1_team}\n        p2_seen_status = {p2_lead['name']: {'hp_pct': 100, 'status': None}}\n        feat_end_boost_diff = 0\n        feat_p1_lead_stay_duration = 0\n        feat_p2_lead_forced_out = 0\n        feat_first_ko_turn = 0\n        feat_p1_setup_moves = 0\n        feat_p2_setup_moves = 0\n        \n        if timeline:\n            feat_num_turns = timeline[-1].get('turn', 0)\n            first_ko_achieved = False\n            last_p2_mon_name = p2_lead['name']\n            \n            for i, turn in enumerate(timeline):\n                p1_state = turn.get('p1_pokemon_state', {})\n                p2_state = turn.get('p2_pokemon_state', {})\n                p1_move = turn.get('p1_move_details')\n                p2_move = turn.get('p2_move_details')\n                \n                # Tracciamento HP/Status (con bug corretto)\n                if p1_state and p1_state.get('name'):\n                    p1_name = p1_state['name']\n                    p1_seen_status.setdefault(p1_name, {'hp_pct': 100, 'status': None})\n                    if p1_state.get('hp_pct') is not None:\n                        p1_seen_status[p1_name]['hp_pct'] = p1_state.get('hp_pct')\n                    p1_seen_status[p1_name]['status'] = p1_state.get('status')\n                if p2_state and p2_state.get('name'):\n                    p2_name = p2_state['name']\n                    p2_seen_status.setdefault(p2_name, {'hp_pct': 100, 'status': None})\n                    if p2_state.get('hp_pct') is not None:\n                        p2_seen_status[p2_name]['hp_pct'] = p2_state.get('hp_pct')\n                    p2_seen_status[p2_name]['status'] = p2_state.get('status')\n                \n                # Boost (solo ultimo turno)\n                if turn.get('turn') == feat_num_turns:\n                    feat_end_boost_diff = sum(p1_state.get('boosts', {}).values()) - sum(p2_state.get('boosts', {}).values())\n                \n                # Tracciamento Durata Lead P1\n                if p1_state.get('name') == p1_lead['name']:\n                    feat_p1_lead_stay_duration += 1\n                \n                # Tracciamento P2 Lead Forzato a Uscire\n                if i > 0 and p2_state.get('name') != last_p2_mon_name and last_p2_mon_name == p2_lead['name']:\n                    feat_p2_lead_forced_out = 1\n                if p2_state.get('name') != last_p2_mon_name:\n                    last_p2_mon_name = p2_state.get('name')\n                \n                # Tracciamento Primo KO\n                if not first_ko_achieved and (p1_state.get('hp_pct') == 0 or p2_state.get('hp_pct') == 0):\n                    feat_first_ko_turn = turn.get('turn', 0)\n                    first_ko_achieved = True\n                \n                # Tracciamento Setup Moves\n                if p1_move and p1_move.get('name') in SETUP_MOVES:\n                    feat_p1_setup_moves += 1\n                if p2_move and p2_move.get('name') in SETUP_MOVES:\n                    feat_p2_setup_moves += 1\n\n        feat_setup_advantage = feat_p1_setup_moves - feat_p2_setup_moves\n\n        # Feature \"Risultato Turno 30\" (le tue migliori)\n        p1_total_hp_seen = sum(p['hp_pct'] for p in p1_seen_status.values())\n        p2_total_hp_seen = sum(p['hp_pct'] for p in p2_seen_status.values())\n        feat_hp_advantage_seen = p1_total_hp_seen - p2_total_hp_seen\n        feat_mons_revealed_diff = len(p2_seen_status) - len(p1_seen_status)\n        \n        def calculate_status_score(status_dict):\n            score = 0\n            for p in status_dict.values():\n                status = p.get('status')\n                if status == 'slp': score += 3\n                elif status == 'par': score += 2\n                elif status in ['psn', 'tox']: score += 1\n            return score\n        p1_team_status_score = calculate_status_score(p1_seen_status)\n        p2_team_status_score = calculate_status_score(p2_seen_status)\n        feat_weighted_status_diff = p1_team_status_score - p2_team_status_score\n\n        processed_data.append({\n            'battle_id': row['battle_id'],\n            'p1_lead_name': p1_lead_name, \n            'p2_lead_name': p2_lead_name,\n            \n            # Statiche (Turno 0)\n            'lead_speed_diff': feat_lead_speed_diff,\n            'p1_team_avg_speed': feat_p1_team_avg_speed,\n            'p1_team_avg_bulk': feat_p1_team_avg_bulk,\n            'p1_meta_threat_count': feat_p1_meta_threat_count,\n            'p2_lead_is_meta_threat': feat_p2_lead_is_meta_threat,\n            \n            # Dinamiche \"Tempo\" (Turno 1-30)\n            'p1_lead_stay_duration': feat_p1_lead_stay_duration,\n            'p2_lead_forced_out': feat_p2_lead_forced_out,\n            'first_ko_turn': feat_first_ko_turn,\n            'setup_advantage': feat_setup_advantage,\n            \n            # Dinamiche \"Risultato\" (Turno 30)\n            'hp_advantage_seen': feat_hp_advantage_seen,\n            'mons_revealed_diff': feat_mons_revealed_diff,\n            'weighted_status_diff': feat_weighted_status_diff,\n            'end_boost_diff': feat_end_boost_diff,\n        })\n    return pd.DataFrame(processed_data).set_index('battle_id')\n\n# Esegui la creazione delle feature\nX_train_features = create_advanced_features_2(train_df)\nX_test_features = create_advanced_features_2(test_df)\ny_train = train_df.set_index('battle_id')['player_won'].loc[X_train_features.index]\nprint(\"Feature engineering ('Full Story') completato.\")\n\n# --- 5. Definizione Liste Feature (TUTTE) ---\nnumeric_features = [\n    # Statiche\n    'lead_speed_diff',\n    'p1_team_avg_speed',\n    'p1_team_avg_bulk',\n    'p1_meta_threat_count',\n    'p2_lead_is_meta_threat',\n    \n    # Dinamiche \"Tempo\"\n    'p1_lead_stay_duration',\n    'p2_lead_forced_out',\n    'first_ko_turn',\n    'setup_advantage',\n    \n    # Dinamiche \"Risultato\"\n    'hp_advantage_seen',\n    'mons_revealed_diff',\n    'weighted_status_diff',\n    'end_boost_diff',\n]\ncategorical_features = ['p1_lead_name', 'p2_lead_name']\n\n# --- 6. Creazione Pipeline (per XGBoost) ---\nprint(\"ðŸ› ï¸  Costruzione pipeline XGBoost...\")\n\nnumeric_transformer = Pipeline(steps=[('scaler', 'passthrough')]) \ncategorical_transformer = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features),\n        ('cat', categorical_transformer, categorical_features)\n    ],\n    remainder='passthrough'\n)\n\ntuning_pipeline_xgb = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('classifier', XGBClassifier(random_state=42, eval_metric='logloss')) \n])\n\n# --- 7. PARAM_GRID (Ottimizzazione Avanzata) --- {'classifier__colsample_bytree': 0.8, 'classifier__gamma': 0.1,\n# 'classifier__learning_rate': 0.05, 'classifier__max_depth': 4, 'classifier__n_estimators': 500, 'classifier__subsample': 0.7}\n#{'classifier__colsample_bytree': 0.9, 'classifier__gamma': 0.2, 'classifier__learning_rate': 0.03, 'classifier__max_depth': 5,\n #'classifier__n_estimators': 600, 'classifier__subsample': 0.8}\nprint(\"âš™ï¸ Definizione della griglia di iperparametri avanzata...\")\nparam_grid = {\n    'classifier__n_estimators': [500, 700],\n    'classifier__learning_rate': [0.02, 0.08],\n    'classifier__max_depth': [3, 5], # Manteniamo la profonditÃ  bassa per evitare overfitting\n    'classifier__subsample': [0.6, 0.9],\n    'classifier__colsample_bytree': [0.75, 1],\n    'classifier__gamma': [0.1, 0.3] # Regolarizzazione piÃ¹ forte\n}\n\n# --- 8. Addestramento GridSearchCV ---\nprint(\"ðŸš€ Inizio addestramento (GridSearchCV 'Full Story')...\")\nprint(\"âš ï¸ Questo richiederÃ  molto tempo!\")\n\ngrid_search = GridSearchCV(\n    tuning_pipeline_xgb, \n    param_grid, \n    cv=5, \n    scoring='accuracy', \n    n_jobs=-1,\n    verbose=2\n)\ngrid_search.fit(X_train_features, y_train)\n\nprint(\"\\n--- âœ… Risultati Finali ---\")\nprint(f\"Migliori parametri trovati: {grid_search.best_params_}\")\nprint(f\"Accuracy FINALE (media CV): {grid_search.best_score_ * 100:.2f}%\")\n# --- 9. Creazione File CSV Finale (submission_predictions.csv) ---\nprint(\"\\nðŸ“„ Creazione file 'submission_predictions.csv'...\")\n\nfinal_model = grid_search.best_estimator_\ntest_predictions = final_model.predict(X_test_features)\ntest_battle_ids = X_test_features.index\n\nsubmission_df = pd.DataFrame({\n    'battle_id': test_battle_ids,\n    'player_won': test_predictions.astype(int)\n})\n\nsubmission_df.to_csv('submission_predictions.csv', index=False)\n\nprint(\"-------------------------------------------------\")\nprint(\"File 'submission_predictions.csv' creato con successo!\")\nprint(\"Lo troverai nel pannello 'Data' -> 'Output' sulla destra.\")\nprint(\"-------------------------------------------------\")\nprint(\"\\nAnteprima del file:\")\nprint(submission_df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T11:26:07.297895Z","iopub.execute_input":"2025-11-13T11:26:07.298164Z","iopub.status.idle":"2025-11-13T11:27:34.580989Z","shell.execute_reply.started":"2025-11-13T11:26:07.298143Z","shell.execute_reply":"2025-11-13T11:27:34.579894Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 2. Creazione Pipeline (per Stacking) ---\nprint(\"ðŸ› ï¸  Costruzione pipeline Stacking...\")\n\n# --- Pipeline 1: XGBoost (Modello non lineare) ---\n# Usa il pre-processore che non standardizza ('passthrough')\npreprocessor_xgb = ColumnTransformer(\n    transformers=[\n        ('num', 'passthrough', numeric_features),\n        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n    ],\n    remainder='passthrough'\n)\n# Usa i parametri migliori che hai trovato per 83.95% #{'classifier__colsample_bytree': 0.9, 'classifier__gamma': 0.2, 'classifier__learning_rate': 0.03, 'classifier__max_depth': 5,\n #'classifier__n_estimators': 600, 'classifier__subsample': 0.8}\n#{'classifier__colsample_bytree': 1, 'classifier__gamma': 0.3, 'classifier__learning_rate': 0.02, 'classifier__max_depth': 5, 'classifier__n_estimators': 700, 'classifier__subsample': 0.9}\n\nclf_xgb = XGBClassifier(random_state=42, \n                        eval_metric='logloss',\n                        n_estimators=600,\n                        learning_rate=0.03,\n                        max_depth=5,\n                        subsample=0.8,\n                        colsample_bytree=0.9,\n                        gamma=0.2)\npipeline_xgb = Pipeline(steps=[('preprocessor', preprocessor_xgb),\n                               ('classifier', clf_xgb)])\n\n# --- Pipeline 2: Logistic Regression (Modello lineare) ---\n# DEVE usare StandardScaler\nnumeric_transformer_logreg = Pipeline(steps=[('scaler', StandardScaler())])\ncategorical_transformer_logreg = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))])\npreprocessor_logreg = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer_logreg, numeric_features),\n        ('cat', categorical_transformer_logreg, categorical_features)\n    ],\n    remainder='passthrough'\n)\n# Usa i parametri migliori che hai trovato per il 73%{'classifier__C': 1, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear'}\nclf_logreg = LogisticRegression(C=10, solver='liblinear', random_state=42, penalty='l1') \npipeline_logreg = Pipeline(steps=[('preprocessor', preprocessor_logreg),\n                                  ('classifier', clf_logreg)])\n\n# --- Livello Finale: Stacking (Il \"Meta-Modello\") ---\nprint(\"ðŸ§  Creazione del Meta-Modello...\")\n# Creiamo il \"Meta-Modello\" (Il Capo)\nmeta_model = LogisticRegression(C=1.0, random_state=42)\n\n# Creiamo il modello Stacked\nstacking_clf = StackingClassifier(\n    estimators=[\n        ('xgb', pipeline_xgb), \n        ('logreg', pipeline_logreg) # Combina i due modelli DIVERSI\n    ], \n    final_estimator=meta_model, # Questo Ã¨ il tuo \"Meta-Modello\"\n    passthrough=False, # Diamo al \"Capo\" solo le previsioni (corregge l'errore 'starmie')\n    cv=5,              # Cruciale per un addestramento robusto\n    n_jobs=-1,\n    verbose=1\n)\n\n# --- 3. Calcolo dell'Accuracy (Cross-Validation) ---\nprint(\"ðŸš€ Avvio della Cross-Validation per il Meta-Modello...\")\nprint(\"Questo richiederÃ  MOLTO tempo...\")\n\nscores = cross_val_score(\n    stacking_clf,                \n    X_train_features,     \n    y_train,              \n    cv=5,                 \n    scoring='accuracy',\n    n_jobs=-1\n)\n\nprint(\"\\n--- âœ… Risultati della Cross-Validation ---\")\nprint(f\"Accuratezza per ognuno dei 5 test: {scores}\")\nprint(f\"Accuracy MEDIA FINALE: {scores.mean() * 100:.2f}%\")\nprint(f\"Deviazione Standard: {scores.std() * 100:.2f}%\")\n\n\n# --- 4. Addestramento Finale (per la Submission) ---\nprint(\"\\nðŸš€ Inizio addestramento finale (Meta-Modello) sull'intero set di training...\")\nstacking_clf.fit(X_train_features, y_train)\nprint(\"Addestramento completato.\")\n\n# --- 5. Creazione File CSV Finale (submission_predictions.csv) ---\nprint(\"\\nðŸ“„ Creazione file 'submission_predictions.csv'...\")\n\nfinal_model = stacking_clf # Il Meta-Modello Ã¨ il modello finale\ntest_predictions = final_model.predict(X_test_features)\ntest_battle_ids = X_test_features.index\n\nsubmission_df = pd.DataFrame({\n    'battle_id': test_battle_ids,\n    'player_won': test_predictions.astype(int)\n})\n\nsubmission_df.to_csv('submission_predictions.csv', index=False)\n\nprint(\"-------------------------------------------------\")\nprint(\"File 'submission_predictions.csv' creato con successo!\")\nprint(\"Lo troverai nel pannello 'Data' -> 'Output' sulla destra.\")\nprint(\"-------------------------------------------------\")\nprint(\"\\nAnteprima del file:\")\nprint(submission_df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T11:36:46.335864Z","iopub.execute_input":"2025-11-13T11:36:46.336141Z","iopub.status.idle":"2025-11-13T11:37:02.865561Z","shell.execute_reply.started":"2025-11-13T11:36:46.336127Z","shell.execute_reply":"2025-11-13T11:37:02.864570Z"}},"outputs":[],"execution_count":null}]}